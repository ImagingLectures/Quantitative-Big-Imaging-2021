%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}




\title{Quantitative Big Imaging - Building and Augmenting Datasets}
\date{Mar 26, 2021}
\release{}
\author{Anders Kaestner}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{03-Datasets::doc}}


\sphinxAtStartPar
\sphinxstylestrong{Quantitative Big Imaging} ETHZ: 227\sphinxhyphen{}0966\sphinxhyphen{}00L




\chapter{Today’s lecture}
\label{\detokenize{03-Datasets:today-s-lecture}}
\sphinxAtStartPar
\sphinxstylestrong{Creating Datasets}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Famous Datasets

\item {} 
\sphinxAtStartPar
Types of Datasets

\item {} 
\sphinxAtStartPar
What makes a good dataet?

\item {} 
\sphinxAtStartPar
Building your own

\item {} 
\sphinxAtStartPar
“scrape, mine, move, annotate, review, and preprocess” \sphinxhyphen{} Kathy Scott

\item {} 
\sphinxAtStartPar
tools to use

\item {} 
\sphinxAtStartPar
simulation

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Augmentation}
\begin{itemize}
\item {} 
\sphinxAtStartPar
How can you artifically increase the size of your dataset?

\item {} 
\sphinxAtStartPar
What are the limits of these increases

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Baselines}
\begin{itemize}
\item {} 
\sphinxAtStartPar
What is a baseline?

\item {} 
\sphinxAtStartPar
Example: Nearest Neighbor

\end{itemize}


\chapter{Let’s load some modules for the notebook}
\label{\detokenize{03-Datasets:let-s-load-some-modules-for-the-notebook}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}    \PYG{k}{as} \PYG{n+nn}{mpl}
\PYG{k+kn}{import} \PYG{n+nn}{numpy}         \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{skimage}       \PYG{k}{as} \PYG{n+nn}{ski}
\PYG{k+kn}{import} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{io}    \PYG{k}{as} \PYG{n+nn}{io}
\PYG{k+kn}{from} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{morphology} \PYG{k+kn}{import} \PYG{n}{disk}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{ndimage} \PYG{k}{as} \PYG{n+nn}{ndimage}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets}  \PYG{k+kn}{import} \PYG{n}{mnist}
\PYG{k+kn}{from} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{util}    \PYG{k+kn}{import} \PYG{n}{montage} \PYG{k}{as} \PYG{n}{montage2d}

\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.dpi}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ModuleNotFoundError}\PYG{g+gWhitespace}{                       }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{ca35de6f9736}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{k+kn}{import} \PYG{n+nn}{matplotlib}    \PYG{k}{as} \PYG{n+nn}{mpl}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} \PYG{k+kn}{import} \PYG{n+nn}{numpy}         \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{4} \PYG{k+kn}{import} \PYG{n+nn}{skimage}       \PYG{k}{as} \PYG{n+nn}{ski}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5} \PYG{k+kn}{import} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{io}    \PYG{k}{as} \PYG{n+nn}{io}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{6} \PYG{k+kn}{from} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{morphology} \PYG{k+kn}{import} \PYG{n}{disk}

\PYG{n+ne}{ModuleNotFoundError}: No module named \PYGZsq{}skimage\PYGZsq{}
\end{sphinxVerbatim}


\chapter{References}
\label{\detokenize{03-Datasets:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://arxiv.org/abs/1707.02968}{Revisiting \sphinxstylestrong{Unreasonable Effectiveness of Data} in Deep Learning Era}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://towardsdatascience.com/data-science-without-any-data-6c1ae9509d92}{Data science … without any data}

\item {} 
\sphinxAtStartPar
Building Datasets
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python Machine Learning 2nd Edition by Sebastian Raschka, Packt Publishing Ltd. 2017

\item {} 
\sphinxAtStartPar
Chapter 2: \sphinxhref{https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch04/ch04.ipynb}{Building Good Datasets:}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://doi.org/10.1007/978-3-319-94878-2\_6}{A Standardised Approach for Preparing Imaging Data for Machine Learning Tasks in Radiology}

\end{itemize}

\item {} 
\sphinxAtStartPar
Creating Datasets / Crowdsourcing

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811917302707}{Mindcontrol: A web application for brain segmentation quality control}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.biorxiv.org/content/10.1101/363382v1.abstract}{Combining citizen science and deep learning to amplify expertise in neuroimaging}

\item {} 
\sphinxAtStartPar
Augmentation tools

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/aleju/imgaug}{ImgAug}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/mdbloice/Augmentor}{Augmentor}

\end{itemize}


\chapter{Motivation}
\label{\detokenize{03-Datasets:motivation}}
\sphinxAtStartPar
\sphinxstylestrong{Why other peoples data?}

\sphinxAtStartPar
Most of you taking this class are rightfully excited to learn about new tools and algorithms to analyzing \sphinxstyleemphasis{your} data.

\sphinxAtStartPar
This lecture is a bit of an anomaly and perhaps disappointment because it doesn’t cover any algorithms, or tools.
\begin{itemize}
\item {} 
\sphinxAtStartPar
You might ask, why are we spending so much time on datasets?

\item {} 
\sphinxAtStartPar
You already collected data (sometimes lots of it) that is why you took this class?!

\end{itemize}

\sphinxAtStartPar
… let’s see what some other people say


\section{Sean Taylor (Research Scientist at Facebook)}
\label{\detokenize{03-Datasets:sean-taylor-research-scientist-at-facebook}}
\sphinxAtStartPar
This tweet tells us that you shouldn’t put too much belief in AI without providing carefully prepared data set. Machine learning methods perform only so good as the data it was trained with. You need a data set that covers all extremes of the fenomena that you want to model.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.5]{{data_tweet}.jpg}
\caption{Realistic thoughts about AI.}\label{\detokenize{03-Datasets:id3}}\end{figure}




\section{Andrej Karpathy (Director of AI at Tesla)}
\label{\detokenize{03-Datasets:andrej-karpathy-director-of-ai-at-tesla}}
\sphinxAtStartPar
This slide by Andrej Karpathy shows the importance of correct data set in a machine learning project. Typically, you should spend much more time on collecting representative data for your models than building the models. Unfortunately, this is not the case for many PhD projects where the data usually is scarse. Much for the reason that it is really hard to come by the data. You may only have a few beam slots allocated for your experiments and this is the data you have to live with.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.7]{{karpathy_slide}.jpg}
\caption{Time spent on different tasks.}\label{\detokenize{03-Datasets:id4}}\end{figure}




\section{Kathy Scott (Image Analytics Lead at Planet Labs)}
\label{\detokenize{03-Datasets:kathy-scott-image-analytics-lead-at-planet-labs}}
\sphinxAtStartPar
Yet another tweet that implies that many data scientist actually spend more time on preparing the data than developing new models.  The training is less labor demanding, the computer is doing that part of the job.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.7]{{kathy_tweet}.png}
\caption{The importance to spend sufficient time on data preparation.}\label{\detokenize{03-Datasets:id5}}\end{figure}




\section{Data is important}
\label{\detokenize{03-Datasets:data-is-important}}
\sphinxAtStartPar
It probably \sphinxhref{https://www.forbes.com/sites/bernardmarr/2018/03/05/heres-why-data-is-not-the-new-oil/}{isn’t the \sphinxstyleemphasis{new} oil}, but it forms an essential component for building modern tools today.

\sphinxAtStartPar
Testing good algorithms \sphinxstyleemphasis{requires} good data
\begin{itemize}
\item {} 
\sphinxAtStartPar
If you don’t know what to expect \sphinxhyphen{} \sphinxstyleemphasis{how do you know your algorithm worked}?

\item {} 
\sphinxAtStartPar
If you have dozens of edge cases \sphinxhyphen{} \sphinxstyleemphasis{how can you make sure it works on each one}?

\item {} 
\sphinxAtStartPar
If a new algorithm is developed every few hours \sphinxhyphen{} \sphinxstyleemphasis{how can you be confident they actually work better}
\begin{itemize}
\item {} 
\sphinxAtStartPar
facebook’s site has a new version multiple times per day and their app every other day

\end{itemize}

\end{itemize}

\sphinxAtStartPar
For machine learning, even building requires good data
\begin{itemize}
\item {} 
\sphinxAtStartPar
If you count cells maybe you can write your own algorithm,

\item {} 
\sphinxAtStartPar
but if you are trying to detect subtle changes in cell structure that indicate cancer you probably can’t write a list of simple mathematical rules yourself.

\end{itemize}


\section{Data is reusable}
\label{\detokenize{03-Datasets:data-is-reusable}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Well organized and structured data is very easy to reuse.

\item {} 
\sphinxAtStartPar
Another project can easily combine your data with their data in order to get even better results.

\end{itemize}

\sphinxAtStartPar
Algorithms are often often only prototypes
\begin{itemize}
\item {} 
\sphinxAtStartPar
messy,

\item {} 
\sphinxAtStartPar
complicated,

\item {} 
\sphinxAtStartPar
poorly written,

\end{itemize}

\sphinxAtStartPar
… especially so if written by students trying to graduate on time.





\sphinxAtStartPar
\sphinxstylestrong{Data recycling saves time and improves performance}






\chapter{Famous Datasets}
\label{\detokenize{03-Datasets:famous-datasets}}
\sphinxAtStartPar
The primary success of datasets has been shown through the most famous datasets collected.

\sphinxAtStartPar
Here I show
\begin{itemize}
\item {} 
\sphinxAtStartPar
Two of the most famous general datasets
\begin{itemize}
\item {} 
\sphinxAtStartPar
MNIST Digits

\item {} 
\sphinxAtStartPar
ImageNET

\end{itemize}

\item {} 
\sphinxAtStartPar
and one of the most famous medical datasets.
\begin{itemize}
\item {} 
\sphinxAtStartPar
BRATS

\end{itemize}

\end{itemize}

\sphinxAtStartPar
The famous datasets are important for basic network training.


\section{MNIST Digits}
\label{\detokenize{03-Datasets:mnist-digits}}
\sphinxAtStartPar
Modified NIST (National Institute of Standards and Technology) created a list of handwritten digits.

\sphinxAtStartPar
This list is a popular starting point for many machine learning projects. The images are already labeled and are also nicely prepared to about the same size and also very high SNR. These properties makes it a great toy data set for first testing.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.5]{{mnist}.png}
\caption{A selection of hand written numbers from the MNIST data base}\label{\detokenize{03-Datasets:id6}}\end{figure}




\section{ImageNet}
\label{\detokenize{03-Datasets:imagenet}}\begin{itemize}
\item {} 
\sphinxAtStartPar
ImageNet is an image database
\begin{itemize}
\item {} 
\sphinxAtStartPar
organized according to the WordNet hierarchy (currently only the nouns),

\item {} 
\sphinxAtStartPar
each node of the hierarchy is depicted by hundreds and thousands of images.

\end{itemize}

\item {} 
\sphinxAtStartPar
1000 different categories and \textgreater{}1M images.

\item {} 
\sphinxAtStartPar
Not just dog/cat, but wolf vs german shepard,

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{A-simplified-WordNet-hierarchy-of-synsets}.png}
\caption{Hierarchial structure of the WordNet database.}\label{\detokenize{03-Datasets:id7}}\end{figure}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{}\PYGZhy{}\PYGZhy{}
scale: 80\PYGZpc{}
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Error rates for different classifiers on the same data set.
\end{sphinxVerbatim}



\sphinxAtStartPar
\sphinxhref{https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5}{CNN architectures}




\section{BRATS}
\label{\detokenize{03-Datasets:brats}}
\sphinxAtStartPar
Segmenting Tumors in Multimodal MRI Brain Images.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.6]{{BRATS_tasks}.png}
\caption{Images of brain tumors from the BRATS database.}\label{\detokenize{03-Datasets:id8}}\end{figure}




\section{What story did these datasets tell?}
\label{\detokenize{03-Datasets:what-story-did-these-datasets-tell}}


\sphinxAtStartPar
Each of these datasets is very different from images with fewer than 1000 pixels to images with more than 100MPx, but what they have in common is how their analysis has changed.

\sphinxAtStartPar
All of these datasets used to be analyzed by domain experts with hand\sphinxhyphen{}crafted features.
\begin{itemize}
\item {} 
\sphinxAtStartPar
A handwriting expert using graph topology to assign images to digits

\item {} 
\sphinxAtStartPar
A computer vision expert using gradients common in faces to identify people in ImageNet

\item {} 
\sphinxAtStartPar
A biomedical engineer using knowledge of different modalities to fuse them together and cluster healthy and tumorous tissue

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.75]{{domainexperts}.pdf}
\caption{Domain experts use their experience to analyze data}\label{\detokenize{03-Datasets:id9}}\end{figure}

\sphinxAtStartPar
Starting in the early 2010s, the approaches of deep learning began to improve and become more computationally efficient. With these techniques groups with \sphinxstylestrong{absolutely no domain knowledge} could begin building algorithms and winning contests based on these datasets.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.75]{{datascientists}.pdf}
\caption{Data scientist don’t have domain specific knowledge, they use available data for the analysis.}\label{\detokenize{03-Datasets:id10}}\end{figure}





\sphinxAtStartPar
All of these datasets used to be analyzed by \sphinxstylestrong{domain experts} with hand\sphinxhyphen{}crafted features.


\begin{itemize}
\item {} 
\sphinxAtStartPar
A handwriting expert using graph topology

\item {} 
\sphinxAtStartPar
A computer vision expert to identify people in ImageNet

\item {} 
\sphinxAtStartPar
A biomedical engineer cluster healthy and tumorous tissue

\end{itemize}


\begin{itemize}
\item {} 
\sphinxAtStartPar
the approaches of deep learning began to improve and become more computationally efficient.

\item {} 
\sphinxAtStartPar
groups with \sphinxstylestrong{absolutely no domain knowledge} could winning contests based on datasets

\end{itemize}






\section{So Deep Learning always wins?}
\label{\detokenize{03-Datasets:so-deep-learning-always-wins}}
\sphinxAtStartPar
No, that isn’t the point of this lecture.

\sphinxAtStartPar
Even if you aren’t using deep learning the point of these stories is having
\begin{itemize}
\item {} 
\sphinxAtStartPar
well\sphinxhyphen{}labeled,

\item {} 
\sphinxAtStartPar
structured,

\item {} 
\sphinxAtStartPar
and organized datasets

\end{itemize}

\sphinxAtStartPar
makes your problem \sphinxstyleemphasis{a lot more accessible} for other groups and enables a variety of different approaches to be tried.

\sphinxAtStartPar
Ultimately it enables better solutions to be made and you to be confident that the solutions are in fact better.


\subsection{The FAIR principle}
\label{\detokenize{03-Datasets:the-fair-principle}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\_\_F\_\_indable

\item {} 
\sphinxAtStartPar
\_\_A\_\_ccessible

\item {} 
\sphinxAtStartPar
\_\_I\_\_nteroperable

\item {} 
\sphinxAtStartPar
\_\_R\_\_eusable

\end{itemize}

\sphinxAtStartPar
\sphinxhref{https://doi.org/10.1038/sdata.2016.18}{Wilkinson et al. 2016}


\section{Other Datasets}
\label{\detokenize{03-Datasets:other-datasets}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{http://Grand-Challenge.org}{Grand\sphinxhyphen{}Challenge.org} a large number of challenges in the biomedical area

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.kaggle.com/datasets}{Kaggle Datasets}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://datasetsearch.research.google.com/}{Google Dataset Search}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/List\_of\_datasets\_for\_machine-learning\_research}{Wikipedia provides a comprehensive list categorized into different topics}

\end{itemize}


\section{What makes a good dataset?}
\label{\detokenize{03-Datasets:what-makes-a-good-dataset}}
\sphinxAtStartPar
A good data set is characterized by
\begin{itemize}
\item {} 
\sphinxAtStartPar
Large amount

\item {} 
\sphinxAtStartPar
Diversity

\item {} 
\sphinxAtStartPar
Annotations

\end{itemize}

\sphinxAtStartPar
This means that…


\subsection{Lots of images}
\label{\detokenize{03-Datasets:lots-of-images}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Small datasets can be useful, but here the bigger the better

\item {} 
\sphinxAtStartPar
Particularly if you have:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Complicated problems

\item {} 
\sphinxAtStartPar
Very subtle differences (ie a lung tumor looks mostly like normal lung tissue but it is in a place it shouldn’t be)

\item {} 
\sphinxAtStartPar
Class imbalance

\end{itemize}

\end{itemize}


\subsection{Lots of diversity}
\label{\detokenize{03-Datasets:lots-of-diversity}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Is it what data ‘in the wild’ really looks like?

\item {} 
\sphinxAtStartPar
Lots of different
\begin{itemize}
\item {} 
\sphinxAtStartPar
Scanners/reconstruction algorithms,

\item {} 
\sphinxAtStartPar
noise levels,

\item {} 
\sphinxAtStartPar
illumination types,

\item {} 
\sphinxAtStartPar
rotation,

\item {} 
\sphinxAtStartPar
colors, …

\end{itemize}

\item {} 
\sphinxAtStartPar
Many examples from different categories
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{if you only have one male with breast cancer it will be hard to generalize exactly what that looks like}

\end{itemize}

\end{itemize}


\subsection{Meaningful labels}
\label{\detokenize{03-Datasets:meaningful-labels}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Clear task or question

\item {} 
\sphinxAtStartPar
Unambiguous (would multiple different labelers come to the same conclusion)

\item {} 
\sphinxAtStartPar
Able to be derived from the image alone
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{A label that someone cannot afford insurance is interesting but it would be nearly impossible to determine that from an X\sphinxhyphen{}ray of their lungs}

\end{itemize}

\item {} 
\sphinxAtStartPar
Quantiative!

\item {} 
\sphinxAtStartPar
Non\sphinxhyphen{}obvious
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{A label saying an image is bright is not a helpful label because you could look at the histogram and say that}

\end{itemize}

\end{itemize}


\chapter{Purpose of different types of Datasets}
\label{\detokenize{03-Datasets:purpose-of-different-types-of-datasets}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Classification

\item {} 
\sphinxAtStartPar
Regression

\item {} 
\sphinxAtStartPar
Segmentation

\item {} 
\sphinxAtStartPar
Detection

\item {} 
\sphinxAtStartPar
Other

\end{itemize}


\section{Classification}
\label{\detokenize{03-Datasets:classification}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.5]{{classificationCD}.pdf}
\caption{Classification example with cats and dogs.}\label{\detokenize{03-Datasets:id11}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Taking an image and putting it into a category

\item {} 
\sphinxAtStartPar
Each image should have exactly one category

\item {} 
\sphinxAtStartPar
The categories should be non\sphinxhyphen{}ordered

\item {} 
\sphinxAtStartPar
Example:

\item {} 
\sphinxAtStartPar
Cat vs Dog

\item {} 
\sphinxAtStartPar
Cancer vs Healthy

\end{itemize}




\subsection{Classification example}
\label{\detokenize{03-Datasets:classification-example}}
\sphinxAtStartPar
In classification you want to observe an image an quickly provide it with a category. Like in the MNIST example which is designed for recognition of handwritten numbers. Each image has a label telling which number it represents.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{c\PYGZus{}ax}\PYG{p}{,} \PYG{n}{c\PYGZus{}img}\PYG{p}{,} \PYG{n}{c\PYGZus{}label} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{c\PYGZus{}img}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{n}{c\PYGZus{}label}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_44_0}.png}


\section{Regression}
\label{\detokenize{03-Datasets:regression}}
\sphinxAtStartPar
Regression almost looks like classification at first sight. You still want to put a number related to the image content. But here it is not strictly bound to the provided categories but rather estimating a value which can be found on the regression line fitted to the data.

\sphinxAtStartPar
Taking an image and predicting one (or more) decimal values
\begin{itemize}
\item {} 
\sphinxAtStartPar
Examples:

\item {} 
\sphinxAtStartPar
Value of a house from the picture taken by owner

\item {} 
\sphinxAtStartPar
Risk of hurricane from satellite image

\end{itemize}


\subsection{Regression example Age from X\sphinxhyphen{}Rays}
\label{\detokenize{03-Datasets:regression-example-age-from-x-rays}}
\sphinxAtStartPar
This dataset contains a collection of X\sphinxhyphen{}ray radiographs of hands. The purpose of the data is to estimate the age of a child based on the radiograph. This can be done using a regression model.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.75]{{bone_age}.png}
\caption{A collection of X\sphinxhyphen{}ray images from children at different ages.}\label{\detokenize{03-Datasets:id12}}\end{figure}



\sphinxAtStartPar
\sphinxhref{https://www.kaggle.com/kmader/attention-on-pretrained-vgg16-for-bone-age}{More details}


\section{Segmentation}
\label{\detokenize{03-Datasets:segmentation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Taking an image and predicting one (or more) values for each pixel

\item {} 
\sphinxAtStartPar
Every pixel needs a label (and a pixel cannot have multiple labels)

\item {} 
\sphinxAtStartPar
Typically limited to a few (\textless{}20) different types of objects

\item {} 
\sphinxAtStartPar
Examples:

\item {} 
\sphinxAtStartPar
Where a tumor is from an image of the lungs?

\item {} 
\sphinxAtStartPar
Where streets are from satellite images of a neighborhood?

\item {} 
\sphinxAtStartPar
Where are the cats and dogs?

\end{itemize}




\subsection{Segmentation example: Nuclei in Microscope Images}
\label{\detokenize{03-Datasets:segmentation-example-nuclei-in-microscope-images}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.5]{{slide}.png}
\caption{Sample with cells.}\label{\detokenize{03-Datasets:id13}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.5]{{labels}.png}
\caption{Labelled cells.}\label{\detokenize{03-Datasets:id14}}\end{figure}



\sphinxAtStartPar
\sphinxhref{https://www.kaggle.com/c/data-science-bowl-2018}{More details on Kaggle}


\section{Detection}
\label{\detokenize{03-Datasets:detection}}
\sphinxAtStartPar
Detection is a combination of segmenation and classification in the sense that the location and extents of a feature is determined and is also categorized into some class. The extents don’t have to be very precise, it is often a bounding box or a convex hull. This coarseness is sufficient for many applications.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Taking an image and predicting where and which type of objects appear

\item {} 
\sphinxAtStartPar
Generally bounding box rather than specific pixels

\item {} 
\sphinxAtStartPar
Multiple objects can overlap

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.6]{{lung_opacity}.png}
\caption{Radiographs to detect opaque regions in X\sphinxhyphen{}Rays}\label{\detokenize{03-Datasets:id15}}\end{figure}


\subsection{Detection example: Opaque Regions in X\sphinxhyphen{}Rays}
\label{\detokenize{03-Datasets:detection-example-opaque-regions-in-x-rays}}
\sphinxAtStartPar
In this example the task is to detect opaque regions in lung X\sphinxhyphen{}ray images to provide a first indication for the physician who should make a diagnosis from the images. The used algorithm marks rectangles on region that are too opaque to be healthy.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{lung_opacity}.png}
\caption{Critical regions detected in lung radiographs.}\label{\detokenize{03-Datasets:id16}}\end{figure}



\sphinxAtStartPar
\sphinxhref{https://www.kaggle.com/c/rsna-pneumonia-detection-challenge}{More details on Kaggle}


\section{Other}
\label{\detokenize{03-Datasets:other}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Unlimited possibilities \sphinxhref{https://junyanz.github.io/CycleGAN/}{here}

\item {} 
\sphinxAtStartPar
Horses to Zebras

\end{itemize}


\subsection{Image Enhancement}
\label{\detokenize{03-Datasets:image-enhancement}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Denoising \sphinxhref{http://cchen156.web.engr.illinois.edu/SID.html}{Learning to See in the Dark}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://data.vision.ee.ethz.ch/cvl/DIV2K/}{Super\sphinxhyphen{}resolution}

\end{itemize}


\chapter{Building your own data sets}
\label{\detokenize{03-Datasets:building-your-own-data-sets}}
\sphinxAtStartPar
Finally, we arrive at your data! As you already have seen, it is a time consuming and labor intense task to collect and prepare data.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Very time consuming

\item {} 
\sphinxAtStartPar
Not a lot of great tools

\item {} 
\sphinxAtStartPar
Very problem specific

\end{itemize}

\sphinxAtStartPar
It is however important to have well\sphinxhyphen{}organized data for the analysis.


\section{Code\sphinxhyphen{}free}
\label{\detokenize{03-Datasets:code-free}}

\subsection{Classification}
\label{\detokenize{03-Datasets:id1}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Organize images into folders

\end{itemize}


\subsection{Regression}
\label{\detokenize{03-Datasets:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Create an excel file (first column image name, next columns to regress)

\end{itemize}


\subsection{Segmentation / Object Detection}
\label{\detokenize{03-Datasets:segmentation-object-detection}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Take \sphinxhref{http://fiji.sc/}{FIJI} or any paint application and manually draw region to be identified and save it as a grayscale image

\end{itemize}


\section{Software for data labelling}
\label{\detokenize{03-Datasets:software-for-data-labelling}}

\subsection{Free tools}
\label{\detokenize{03-Datasets:free-tools}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Classification / Segmentation: \sphinxurl{https://github.com/Labelbox/Labelbox}

\item {} 
\sphinxAtStartPar
Classification/ Object Detection: \sphinxurl{http://labelme.csail.mit.edu/Release3.0/}

\item {} 
\sphinxAtStartPar
Classification: \sphinxurl{https://github.com/janfreyberg/superintendent:} \sphinxurl{https://www.youtube.com/watch?v=fMg0mPYiEx0}

\item {} 
\sphinxAtStartPar
Classification/ Detection: \sphinxurl{https://github.com/chestrays/jupyanno:} \sphinxurl{https://www.youtube.com/watch?v=XDIJU5Beg\_w}

\item {} 
\sphinxAtStartPar
Classification (Tinder for Brain MRI): \sphinxurl{https://braindr.us/\#/}

\end{itemize}


\subsection{Commercial Approaches}
\label{\detokenize{03-Datasets:commercial-approaches}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxurl{https://www.figure-eight.com/}

\item {} 
\sphinxAtStartPar
MightyAI / Spare5: \sphinxurl{https://mighty.ai/} \sphinxurl{https://app.spare5.com/fives/sign\_in}

\end{itemize}


\subsection{Example: annotation of spots}
\label{\detokenize{03-Datasets:example-annotation-of-spots}}
\sphinxAtStartPar
Spots are outliers in radiography and very annoying when the images are used for tomography.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{markedspots}.pdf}
\caption{Annotation of spot in a neutron radiograph.}\label{\detokenize{03-Datasets:id17}}\end{figure}


\begin{itemize}
\item {} 
\sphinxAtStartPar
Image size 2048x2048

\item {} 
\sphinxAtStartPar
Tools
\begin{itemize}
\item {} 
\sphinxAtStartPar
Bitmap painting application

\item {} 
\sphinxAtStartPar
Drawing tablet

\end{itemize}

\item {} 
\sphinxAtStartPar
Time to markup \sphinxstyleemphasis{8h}

\end{itemize}


\section{Simulations}
\label{\detokenize{03-Datasets:simulations}}
\sphinxAtStartPar
A further way to increase training data is to build a model of the features you want to train on. This approach has the advantage that you know where to look for the features which means the tedious annotation task is reduced to a minimum. The work rather lies in building a relieable model that should reflect the characteristics of features you want to segments. Once a valid model is built, it is easy to generate masses of data under variuos conditions.

\sphinxAtStartPar
Simulations can be done using:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Geometric models

\item {} 
\sphinxAtStartPar
Template models

\item {} 
\sphinxAtStartPar
Physical models

\end{itemize}

\sphinxAtStartPar
Both augmented and simulated data should be combined with real data.


\subsection{Simulation examples}
\label{\detokenize{03-Datasets:simulation-examples}}
\sphinxAtStartPar
Another way to enhance or expand your dataset is to use simulations
\begin{itemize}
\item {} 
\sphinxAtStartPar
already incorporate realistic data (game engines, 3D rendering, physics models)

\item {} 
\sphinxAtStartPar
100\% accurate ground truth (original models)

\item {} 
\sphinxAtStartPar
unlimited, risk\sphinxhyphen{}free playability (driving cars in the world is more dangerous)

\end{itemize}


\subsubsection{Examples}
\label{\detokenize{03-Datasets:examples}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://pdfs.semanticscholar.org/30a1/ba9142b9c3b755da2bff7d93d704494fdaed.pdf}{P. Fuchs et al. \sphinxhyphen{} Generating Meaningful Synthetic Ground Truth for
Pore Detection in Cast Aluminum Parts, iCT 2019, Padova}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://download.visinf.tu-darmstadt.de/data/from\_games/}{Playing for Data: Ground Truth from Computer Games}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://pythonprogramming.net/self-driving-car-neural-network-training-data-python-plays-gta-v/}{Self\sphinxhyphen{}driving car}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://towardsdatascience.com/learning-from-simulated-data-ff4be63ac89c}{Learning from simulated data}

\end{itemize}


\chapter{Dataset Problems}
\label{\detokenize{03-Datasets:dataset-problems}}
\sphinxAtStartPar
Some of the issues which can come up with datasets are
\begin{itemize}
\item {} 
\sphinxAtStartPar
imbalance

\item {} 
\sphinxAtStartPar
too few examples

\item {} 
\sphinxAtStartPar
too homogenous

\item {} 
\sphinxAtStartPar
and other possible problems

\end{itemize}

\sphinxAtStartPar
These lead to problems with the algorithms built on top of them.


\section{Bias}
\label{\detokenize{03-Datasets:bias}}
\sphinxAtStartPar
Working with single sided data will bias the model towards this kind of data. This is a reason for the need to include as many corner cases as possible in the data. Biasing can easily happen when you have too few data to provide a statistically well founded training.

\sphinxAtStartPar
The gorilla example may sound fun but it can also upset people and in some cases the wrong descision can even cause inrepairable damage. Google’s quick fix to the problem was to remove the gorilla category from their classifyer. This approach may work for a trivial service like picture categorization tool, but yet again what if it is an essential category for the model?

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.6]{{google-racist-gorilla-doctored-tweet}.png}
\caption{Mistakes that can happen due bias caused by insufficent training data.}\label{\detokenize{03-Datasets:id18}}\end{figure}





\sphinxAtStartPar
\sphinxhref{https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai}{The solution was to remove Gorilla from the category}





\subsection{A better solution to avoid biasing}
\label{\detokenize{03-Datasets:a-better-solution-to-avoid-biasing}}
\sphinxAtStartPar
\sphinxstylestrong{Use training sets with more diverse people}

\sphinxAtStartPar
The better solution to avoid biasing mistakes is to use a large data base with more variations one example is the \sphinxhref{https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/}{IBM Diverse Face Dataset}. This face dataset not only provides great variation in people but also adds features to categorize the pictures even further. The figure below shows some samples from the face dataset with categories like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Accessories like eyeglases and hats

\item {} 
\sphinxAtStartPar
Different hair styles

\item {} 
\sphinxAtStartPar
Face shapes

\item {} 
\sphinxAtStartPar
Face expressions

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{celeb_dataset}.png}
\caption{Use a database with more diverse people to avoid biasing.}\label{\detokenize{03-Datasets:id19}}\end{figure}



\sphinxAtStartPar
IBM Diverse Face Dataset


\section{Image data and labels}
\label{\detokenize{03-Datasets:image-data-and-labels}}
\sphinxAtStartPar
In the prevoius example with face pictures we started to look into categories of pictures. These pictures were provided with labels bescribing the picture content. The next dataset we will look at is the MNIST data set, which we already have seen a couple of times in this lecture.

\sphinxAtStartPar
In the example below we have extracted the numbers 1,2, and 3. The histogram to the right shows the distribution of the numbers in the extracted data set.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{d\PYGZus{}subset} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{in1d}\PYG{p}{(}\PYG{n}{label}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{montage2d}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{d\PYGZus{}subset}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{64}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{d\PYGZus{}subset}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{64}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}          \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Digit Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_86_0}.png}


\section{Limited data}
\label{\detokenize{03-Datasets:limited-data}}
\sphinxAtStartPar
Machine learning methods require a lot of training data to be able to build good models that are able to detect the features they are intended to.

\sphinxAtStartPar
\sphinxstyleemphasis{Different types of limited data}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Few data points or limited amounts of images

\end{itemize}

\sphinxAtStartPar
This is very often the case in neutron imaging. The number of images collected during an experiment session is often very low due to the long experiment duration and limited amount of beam time. This makes it hard to develop segmentation and analysis methods for single experiments. The few data points problem can partly be overcome by using data from previous experiment with similar characteristics. The ability to recycle data depends on what you want to detect in the images.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Unbalanced data

\end{itemize}

\sphinxAtStartPar
Unbalanced data means that the ratio between the data points with features you want detect and the total number data points is several orders of magnitude. E.g roots in a volume like the example we will look at later in this lecture. There is even a risk that the distribution of the wanted features is overlapped by the dominating background distribution.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{classunbalance}.pdf}
\caption{Two cases of unblanaced data; (a) the classes are well separated and the feature class is clearly visible in the tail distribution of the background and (b) the feature class is embeded in the background making it hard to detect.}\label{\detokenize{03-Datasets:id20}}\end{figure}

\sphinxAtStartPar
Case (a) can most likely be segmented using one of the many histogram based thresholding methods proposed in literature. Case (b) is much harder to segment as the target features have similar gray levels as the background. This case requires additional information to make segmentation posible.


\begin{itemize}
\item {} 
\sphinxAtStartPar
Little or missing training data

\end{itemize}

\sphinxAtStartPar
A complete set of training data contains both input data and labelled data. The input data is easy to obtain, it is the images you measured during your experiment. The labelled data is harder to get as it is a kind of chicken and egg problem. In particular, if your experiment data is limited. In that case, you would have to mark\sphinxhyphen{}up most of the available data to obtain the labeled data. Which doesn’t make sense because
\begin{itemize}
\item {} 
\sphinxAtStartPar
then you’d already solved the task before even starting to train your segmentation algorithm.

\item {} 
\sphinxAtStartPar
An algorithm based on learning doesn’t improve the results, it only make it easier to handle large amounts of data.

\end{itemize}


\chapter{Augmentation}
\label{\detokenize{03-Datasets:augmentation}}
\sphinxAtStartPar
Obtaining more experiment data is mostly relatively hard,
\begin{itemize}
\item {} 
\sphinxAtStartPar
Time in the lab is limited.

\item {} 
\sphinxAtStartPar
Sample preparation is expensive.

\item {} 
\sphinxAtStartPar
The number of specimens is limited.

\end{itemize}

\sphinxAtStartPar
Still, many supervised analysis methods require large data sets to perform reliably. A method to improve this situation is to use data augmentation. This means that you take the existing data and distorts it using different transformations or add features.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Most groups have too little well\sphinxhyphen{}labeled data and labeling new examples can be very expensive.

\item {} 
\sphinxAtStartPar
Additionally there might not be very many cases of specific classes.

\item {} 
\sphinxAtStartPar
In medicine this is particularly problematic, because some diseases might only happen a few times in a given hospital and you still want to be able to recognize the disease and not that particular person.

\end{itemize}


\section{Typical augmentation operations}
\label{\detokenize{03-Datasets:typical-augmentation-operations}}

\subsection{Transformations}
\label{\detokenize{03-Datasets:transformations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Shift

\item {} 
\sphinxAtStartPar
Zoom

\item {} 
\sphinxAtStartPar
Rotation

\item {} 
\sphinxAtStartPar
Intensity

\item {} 
\sphinxAtStartPar
Normalization

\item {} 
\sphinxAtStartPar
Scaling

\item {} 
\sphinxAtStartPar
Color

\item {} 
\sphinxAtStartPar
Shear

\end{itemize}


\subsection{Further modifications}
\label{\detokenize{03-Datasets:further-modifications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Add noise

\item {} 
\sphinxAtStartPar
Blurring

\end{itemize}


\section{Some augmentation examples}
\label{\detokenize{03-Datasets:some-augmentation-examples}}
\sphinxAtStartPar
The figure below shows some examples of augmentations of the same image. You can also add noise and modulate the image intensity to increase the variations further.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{Augmentations}.pdf}
\caption{A retinal image modified using different augmentation techniques (source: \sphinxurl{https://drive.grand-challenge.org/DRIVE/}) prepared by Gian Guido Parenza.}\label{\detokenize{03-Datasets:id21}}\end{figure}



\sphinxAtStartPar
Retial images from \sphinxhref{https://drive.grand-challenge.org/DRIVE/}{DRIVE} prepared by Gian Guido Parenza.




\section{Limitations of augmentation}
\label{\detokenize{03-Datasets:limitations-of-augmentation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
What transformations are normal in the images?

\item {} 
\sphinxAtStartPar
CT images usually do not get flipped (the head is always on the top)

\item {} 
\sphinxAtStartPar
The values in CT images have a physical meaning (Hounsfield unit),  \(\rightarrow\) scaling them changes the image

\item {} 
\sphinxAtStartPar
How much distortion is too much?

\item {} 
\sphinxAtStartPar
Can you still recognize the features?

\end{itemize}


\section{Keras ImageDataGenerator}
\label{\detokenize{03-Datasets:keras-imagedatagenerator}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
ImageDataGenerator(
    [\PYGZsq{}featurewise\PYGZus{}center=False\PYGZsq{}, \PYGZsq{}samplewise\PYGZus{}center=False\PYGZsq{}, \PYGZsq{}featurewise\PYGZus{}std\PYGZus{}normalization=False\PYGZsq{}, \PYGZsq{}samplewise\PYGZus{}std\PYGZus{}normalization=False\PYGZsq{}, \PYGZsq{}zca\PYGZus{}whitening=False\PYGZsq{}, \PYGZsq{}zca\PYGZus{}epsilon=1e\PYGZhy{}06\PYGZsq{}, \PYGZsq{}rotation\PYGZus{}range=0.0\PYGZsq{}, \PYGZsq{}width\PYGZus{}shift\PYGZus{}range=0.0\PYGZsq{}, \PYGZsq{}height\PYGZus{}shift\PYGZus{}range=0.0\PYGZsq{}, \PYGZsq{}shear\PYGZus{}range=0.0\PYGZsq{}, \PYGZsq{}zoom\PYGZus{}range=0.0\PYGZsq{}, \PYGZsq{}channel\PYGZus{}shift\PYGZus{}range=0.0\PYGZsq{}, \PYGZdq{}fill\PYGZus{}mode=\PYGZsq{}nearest\PYGZsq{}\PYGZdq{}, \PYGZsq{}cval=0.0\PYGZsq{}, \PYGZsq{}horizontal\PYGZus{}flip=False\PYGZsq{}, \PYGZsq{}vertical\PYGZus{}flip=False\PYGZsq{}, \PYGZsq{}rescale=None\PYGZsq{}, \PYGZsq{}preprocessing\PYGZus{}function=None\PYGZsq{}, \PYGZsq{}data\PYGZus{}format=None\PYGZsq{}],
)
Docstring:     
Generate minibatches of image data with real\PYGZhy{}time data augmentation.

\PYGZsh{} Arguments
    featurewise\PYGZus{}center: set input mean to 0 over the dataset.
    samplewise\PYGZus{}center: set each sample mean to 0.
    featurewise\PYGZus{}std\PYGZus{}normalization: divide inputs by std of the dataset.
    samplewise\PYGZus{}std\PYGZus{}normalization: divide each input by its std.
    zca\PYGZus{}whitening: apply ZCA whitening.
    zca\PYGZus{}epsilon: epsilon for ZCA whitening. Default is 1e\PYGZhy{}6.
    rotation\PYGZus{}range: degrees (0 to 180).
    width\PYGZus{}shift\PYGZus{}range: fraction of total width, if \PYGZlt{} 1, or pixels if \PYGZgt{}= 1.
    height\PYGZus{}shift\PYGZus{}range: fraction of total height, if \PYGZlt{} 1, or pixels if \PYGZgt{}= 1.
    shear\PYGZus{}range: shear intensity (shear angle in degrees).
    zoom\PYGZus{}range: amount of zoom. if scalar z, zoom will be randomly picked
        in the range [1\PYGZhy{}z, 1+z]. A sequence of two can be passed instead
        to select this range.
    channel\PYGZus{}shift\PYGZus{}range: shift range for each channel.
    fill\PYGZus{}mode: points outside the boundaries are filled according to the
        given mode (\PYGZsq{}constant\PYGZsq{}, \PYGZsq{}nearest\PYGZsq{}, \PYGZsq{}reflect\PYGZsq{} or \PYGZsq{}wrap\PYGZsq{}). Default
        is \PYGZsq{}nearest\PYGZsq{}.
        Points outside the boundaries of the input are filled according to the given mode:
            \PYGZsq{}constant\PYGZsq{}: kkkkkkkk|abcd|kkkkkkkk (cval=k)
            \PYGZsq{}nearest\PYGZsq{}:  aaaaaaaa|abcd|dddddddd
            \PYGZsq{}reflect\PYGZsq{}:  abcddcba|abcd|dcbaabcd
            \PYGZsq{}wrap\PYGZsq{}:  abcdabcd|abcd|abcdabcd
    cval: value used for points outside the boundaries when fill\PYGZus{}mode is
        \PYGZsq{}constant\PYGZsq{}. Default is 0.
    horizontal\PYGZus{}flip: whether to randomly flip images horizontally.
    vertical\PYGZus{}flip: whether to randomly flip images vertically.
    rescale: rescaling factor. If None or 0, no rescaling is applied,
        otherwise we multiply the data by the value provided. This is
        applied after the `preprocessing\PYGZus{}function` (if any provided)
        but before any other transformation.
    preprocessing\PYGZus{}function: function that will be implied on each input.
        The function will run before any other modification on it.
        The function should take one argument:
        one image (Numpy tensor with rank 3),
\end{sphinxVerbatim}


\subsection{A Keras ImageDataGenerator example}
\label{\detokenize{03-Datasets:a-keras-imagedatagenerator-example}}
\sphinxAtStartPar
There are quite many degrees of freedom to use the ImageDataGenerator. The generator is given all boundary condition at initialization time. Below you see an example of how it can be initialized.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{mnist}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing}\PYG{n+nn}{.}\PYG{n+nn}{image} \PYG{k+kn}{import} \PYG{n}{ImageDataGenerator}

\PYG{n}{img\PYGZus{}aug} \PYG{o}{=} \PYG{n}{ImageDataGenerator}\PYG{p}{(}
    \PYG{n}{featurewise\PYGZus{}center}  \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{samplewise\PYGZus{}center}   \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{zca\PYGZus{}whitening}       \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{zca\PYGZus{}epsilon}         \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}06}\PYG{p}{,}
    \PYG{n}{rotation\PYGZus{}range}      \PYG{o}{=} \PYG{l+m+mf}{30.0}\PYG{p}{,}
    \PYG{n}{width\PYGZus{}shift\PYGZus{}range}   \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{height\PYGZus{}shift\PYGZus{}range}  \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{shear\PYGZus{}range}         \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{zoom\PYGZus{}range}          \PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{,}
    \PYG{n}{fill\PYGZus{}mode}           \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nearest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{horizontal\PYGZus{}flip}     \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{vertical\PYGZus{}flip}       \PYG{o}{=} \PYG{k+kc}{False}
\PYG{p}{)}
\end{sphinxVerbatim}


\section{Augmenting MNIST images}
\label{\detokenize{03-Datasets:augmenting-mnist-images}}
\sphinxAtStartPar
Even something as simple as labeling digits can be very time consuming (maybe 1\sphinxhyphen{}2 per second).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{mnist}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;} \PYG{n}{img} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} setup augmentation}
\PYG{n}{img\PYGZus{}aug}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{img}\PYG{p}{)}
\PYG{n}{real\PYGZus{}aug} \PYG{o}{=} \PYG{n}{img\PYGZus{}aug}\PYG{o}{.}\PYG{n}{flow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{c\PYGZus{}axs}\PYG{p}{,} \PYG{n}{do\PYGZus{}augmentation} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{p}{,} \PYG{p}{[}\PYG{k+kc}{False}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{do\PYGZus{}augmentation}\PYG{p}{:}
        \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{real\PYGZus{}aug}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch} \PYG{o}{=} \PYG{n}{img}\PYG{p}{,} \PYG{n}{label}
    \PYG{k}{for} \PYG{n}{c\PYGZus{}ax}\PYG{p}{,} \PYG{n}{c\PYGZus{}img}\PYG{p}{,} \PYG{n}{c\PYGZus{}label} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{c\PYGZus{}axs}\PYG{p}{,} \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{c\PYGZus{}img}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{255}\PYG{p}{)}
        \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(} \PYG{n}{c\PYGZus{}label}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{aug}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{do\PYGZus{}augmentation} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_111_0}.png}


\section{A larger open data set}
\label{\detokenize{03-Datasets:a-larger-open-data-set}}
\sphinxAtStartPar
We can use a more exciting dataset to try some of the other features in augmentation.

\sphinxAtStartPar
The CIFAR\sphinxhyphen{}10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

\sphinxAtStartPar
Here are some examples from the \sphinxhref{https://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR10} dataset




\subsection{Augmenting CIFAR10 images}
\label{\detokenize{03-Datasets:augmenting-cifar10-images}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{cifar10}
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{cifar10}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{img\PYGZus{}aug} \PYG{o}{=} \PYG{n}{ImageDataGenerator}\PYG{p}{(}
    \PYG{n}{featurewise\PYGZus{}center}  \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{samplewise\PYGZus{}center}   \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{zca\PYGZus{}whitening}       \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{zca\PYGZus{}epsilon}         \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}06}\PYG{p}{,}
    \PYG{n}{rotation\PYGZus{}range}      \PYG{o}{=} \PYG{l+m+mf}{30.0}\PYG{p}{,}
    \PYG{n}{width\PYGZus{}shift\PYGZus{}range}   \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{height\PYGZus{}shift\PYGZus{}range}  \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{channel\PYGZus{}shift\PYGZus{}range} \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{shear\PYGZus{}range}         \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{n}{zoom\PYGZus{}range}          \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{n}{fill\PYGZus{}mode}           \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reflect}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{horizontal\PYGZus{}flip}     \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{vertical\PYGZus{}flip}       \PYG{o}{=} \PYG{k+kc}{True}
\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Running the CIFAR augmentation}
\label{\detokenize{03-Datasets:running-the-cifar-augmentation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{mnist}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} setup augmentation}
\PYG{n}{img\PYGZus{}aug}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{img}\PYG{p}{)}
\PYG{n}{real\PYGZus{}aug} \PYG{o}{=} \PYG{n}{img\PYGZus{}aug}\PYG{o}{.}\PYG{n}{flow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{c\PYGZus{}axs}\PYG{p}{,} \PYG{n}{do\PYGZus{}augmentation} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{p}{,} \PYG{p}{[}\PYG{k+kc}{False}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{do\PYGZus{}augmentation}\PYG{p}{:}
        \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{real\PYGZus{}aug}\PYG{p}{)}
        \PYG{n}{img\PYGZus{}batch} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{img\PYGZus{}batch}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{img\PYGZus{}batch} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{clip}\PYG{p}{(}\PYG{n}{img\PYGZus{}batch}\PYG{o}{/}\PYG{n}{img\PYGZus{}batch}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*}
                            \PYG{l+m+mi}{255}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{255}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{uint8}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch} \PYG{o}{=} \PYG{n}{img}\PYG{p}{,} \PYG{n}{label}
    \PYG{k}{for} \PYG{n}{c\PYGZus{}ax}\PYG{p}{,} \PYG{n}{c\PYGZus{}img}\PYG{p}{,} \PYG{n}{c\PYGZus{}label} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{c\PYGZus{}axs}\PYG{p}{,} \PYG{n}{img\PYGZus{}batch}\PYG{p}{,} \PYG{n}{label\PYGZus{}batch}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{c\PYGZus{}img}\PYG{p}{)}
        \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}
            \PYG{n}{c\PYGZus{}label}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{aug}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{do\PYGZus{}augmentation} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_120_0}.png}


\chapter{Baselines}
\label{\detokenize{03-Datasets:baselines}}\begin{itemize}
\item {} 
\sphinxAtStartPar
A baseline is
\begin{itemize}
\item {} 
\sphinxAtStartPar
a simple,

\item {} 
\sphinxAtStartPar
easily implemented and understood model

\item {} 
\sphinxAtStartPar
that illustrates the problem

\item {} 
\sphinxAtStartPar
and the ‘worst\sphinxhyphen{}case scenario’ for a model that learns nothing (some models will do worse, but these are especially useless).

\end{itemize}

\item {} 
\sphinxAtStartPar
Why is this important?

\end{itemize}


\section{Baseline model example}
\label{\detokenize{03-Datasets:baseline-model-example}}
\sphinxAtStartPar
I have a a model that is \textgreater{}99\% accurate for predicting breast cancer:
\begin{equation*}
\begin{split} \textrm{DoIHaveBreastCancer}(\textrm{Age}, \textrm{Weight}, \textrm{Race}) = \textrm{No!} \end{split}
\end{equation*}



\section{The dummy classifier}
\label{\detokenize{03-Datasets:the-dummy-classifier}}
\sphinxAtStartPar
Let’s train the dummy classifier with some values related to healthy and cancer sick patients. Measurements values 0,1, and 2 are healthy while the value 3 has cancer. We train the classifyer with the strategy that the most frequent class will predict the outcome.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{dummy} \PYG{k+kn}{import} \PYG{n}{DummyClassifier}

\PYG{n}{dc} \PYG{o}{=} \PYG{n}{DummyClassifier}\PYG{p}{(}\PYG{n}{strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{most\PYGZus{}frequent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{dc}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} 
       \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Healthy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Healthy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Healthy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cancer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
DummyClassifier(strategy=\PYGZsq{}most\PYGZus{}frequent\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Testing the outcome of the classifyer}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{idx} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{]} \PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction for }\PYG{l+s+si}{\PYGZob{}0\PYGZcb{}}\PYG{l+s+s1}{ is }\PYG{l+s+si}{\PYGZob{}1\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{idx}\PYG{p}{,}\PYG{n}{dc}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Prediction for 0 is Healthy
Prediction for 1 is Healthy
Prediction for 3 is Healthy
Prediction for 100 is Healthy
\end{sphinxVerbatim}

\sphinxAtStartPar
With these few lines we test what happens when we provide some numbers to the classifyer. The numbers are
\begin{itemize}
\item {} 
\sphinxAtStartPar
0 and 1, which are expected to be healthy

\item {} 
\sphinxAtStartPar
3 , which has cancer

\item {} 
\sphinxAtStartPar
100, unknown to the model

\end{itemize}

\sphinxAtStartPar
So, the classifyer tells us that all values are from healthy patients… not really good! The reason is that it was told to tell us the category of the majority, which is that the patient is healthy.


\section{Try dummy classifier on MNIST data}
\label{\detokenize{03-Datasets:try-dummy-classifier-on-mnist-data}}
\sphinxAtStartPar
The previous basic problem showed us how the dummy classifier work. Now we want to use it with the handwritten numbers in the MNIST dataset. The first step is to load the data and check how the distribution of numbers in the data set using a histogram.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{mnist}
\PYG{k+kn}{from} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{util} \PYG{k+kn}{import} \PYG{n}{montage} \PYG{k}{as} \PYG{n}{montage2d}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;} \PYG{n}{m\PYGZus{}axs}\PYG{o}{=} \PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m\PYGZus{}axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Digit Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;} \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_132_0}.png}


\subsection{Let’s train the model…}
\label{\detokenize{03-Datasets:let-s-train-the-model}}
\sphinxAtStartPar
Now we want to train the model with our data. Once again we use the \sphinxstyleemphasis{most frequent} model. The training is done in the first 24 images in the data set. The fitting requires that we provide the images with numbers and their associated labels telling the model how to interpret the image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dc} \PYG{o}{=} \PYG{n}{DummyClassifier}\PYG{p}{(}\PYG{n}{strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{most\PYGZus{}frequent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dc}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
DummyClassifier(strategy=\PYGZsq{}most\PYGZus{}frequent\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{A basic test}

\sphinxAtStartPar
In the basic test, we provide the first ten images and hope to get predictions which numbers they represent.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dc}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)
\end{sphinxVerbatim}


\subsection{Test on the images}
\label{\detokenize{03-Datasets:test-on-the-images}}
\sphinxAtStartPar
Let’s see how good these predictions really are by showing the images along with their labels and the prediction of the trained model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{prediction} \PYG{o}{=} \PYG{n}{dc}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Predicted: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{prediction}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{prediction} \PYG{o}{==} \PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_141_0}.png}


\subsection{… why are all predictions = 1?}
\label{\detokenize{03-Datasets:why-are-all-predictions-1}}
\sphinxAtStartPar
The result of the basic classifyer was quite disapointing. It told us that all ten images contained the number ‘1’. Now, why is that?

\sphinxAtStartPar
This can be explained by looking at the label histogram:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency of numbers in the training data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_144_0}.png}

\sphinxAtStartPar
Here, we see that there are most ‘1’s in the training data. We have been using the \sphinxstyleemphasis{most frequent} model for training the classifyer therefore the response ‘1’ is the only answer the model can give us.


\section{Nearest Neighbor}
\label{\detokenize{03-Datasets:nearest-neighbor}}
\sphinxAtStartPar
A better baseline

\sphinxAtStartPar
This isn’t a machine learning class and so we won’t dive deeply into other methods, but nearest neighbor is often a very good baseline (that is also very easy to understand). You basically take the element from the original set that is closest to the image you show.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{Russ_fig12_58}.png}
\caption{Examples of the k\sphinxhyphen{}nearest neighbors classifyer (\sphinxhref{https://www.crcpress.com/The-Image-Processing-Handbook/Russ-Neal/p/book/9781138747494}{Figure from J. Russ, Image Processing Handbook}).}\label{\detokenize{03-Datasets:id22}}\end{figure}



\sphinxAtStartPar
Figure from J. Russ, Image Processing Handbook

\sphinxAtStartPar
You can make the method more robust by using more than one nearest neighbor (hence K nearest neighbors), but that we will cover later in the supervised methods lecture.


\subsection{Let’s load the data again…}
\label{\detokenize{03-Datasets:let-s-load-the-data-again}}
\sphinxAtStartPar
Let’s come back to the MNIST numbers again. This time, we will try the k\sphinxhyphen{}nearest neighbors as baseline and see if we can get a better result than with the dummy classifyer with majority voting.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{mnist}
\PYG{k+kn}{from} \PYG{n+nn}{skimage}\PYG{n+nn}{.}\PYG{n+nn}{util} \PYG{k+kn}{import} \PYG{n}{montage} \PYG{k}{as} \PYG{n}{montage2d}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{img}\PYG{p}{,} \PYG{n}{label}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{m\PYGZus{}axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{m\PYGZus{}axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Digit Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;} \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_153_0}.png}


\subsection{Training k\sphinxhyphen{}nearest neighbors}
\label{\detokenize{03-Datasets:training-k-nearest-neighbors}}
\sphinxAtStartPar
The training of the k\sphinxhyphen{}nearest neigbors consists of filling feature vectors into the model and assign each vector to a class.

\sphinxAtStartPar
But images are not vectors… so what we do is to rearrange the \(N\times{}M\) images into a vector with the dimensions \(M\cdot{}N\times{}1\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{n}{neigh\PYGZus{}class} \PYG{o}{=} \PYG{n}{KNeighborsClassifier}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{24}
\PYG{n}{neigh\PYGZus{}class}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{p}{:}\PYG{n}{N}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{p}{[}\PYG{p}{:}\PYG{n}{N}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
KNeighborsClassifier(n\PYGZus{}neighbors=3)
\end{sphinxVerbatim}


\subsection{Predict on a few images}
\label{\detokenize{03-Datasets:predict-on-a-few-images}}
\sphinxAtStartPar
The prediction of which class an image belongs to is done by reshaping the input image into a vector in the same manner as for the training data. Now we will compare the input vector \(u\) to all the vectors in the trained model \(v_i\) by computing the Euclidean distance between the vectors. This can easily be done by the inner product of the two vectors:
\begin{equation*}
\begin{split}D_i=(v_i-u)^T \cdot{} (v_i-u) = scalar\end{split}
\end{equation*}
\sphinxAtStartPar
The class is chosen by the model vector that is closest to the input vector, i.e. having the smallest \(D_i\). This calculations are done for you as a black box in the \sphinxcode{\sphinxupquote{KNeighborsClassifier}}, you only have to reshape the images into the right format.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{neigh\PYGZus{}class}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([3, 0, 4, 1, 9, 9, 1, 3, 1, 9], dtype=uint8)
\end{sphinxVerbatim}


\subsection{Compare predictions with the images}
\label{\detokenize{03-Datasets:compare-predictions-with-the-images}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    
    \PYG{n}{prediction} \PYG{o}{=} \PYG{n}{neigh\PYGZus{}class}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Predicted: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{prediction}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{prediction} \PYG{o}{==} \PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_161_0}.png}

\sphinxAtStartPar
Wow, 100\% correct!


\subsection{100\% for a baseline !!?}
\label{\detokenize{03-Datasets:for-a-baseline}}
\sphinxAtStartPar
Wow the model works really really well, it got every example perfectly.

\sphinxAtStartPar
What we did here (a common mistake) was evaluate on the same data we ‘trained’ on which means the model just correctly recalled each example. This is natural as there is always an image that gives the distance \(D_i=0\).

\sphinxAtStartPar
Now, if we try it on new images we can see the performance drop but still a somewhat reasonable result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{m\PYGZus{}axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{c\PYGZus{}ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{m\PYGZus{}axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{prediction} \PYG{o}{=} \PYG{n}{neigh\PYGZus{}class}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{;} 
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Predicted: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{prediction}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{prediction} \PYG{o}{==} \PYG{n}{label}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{c\PYGZus{}ax}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_165_0}.png}




\section{How good is good?}
\label{\detokenize{03-Datasets:how-good-is-good}}
\sphinxAtStartPar
From the previous example, we saw that the classify doesn’t really reach the 100\% accuracy on unseen data, but rather makes a mistake here or there. Therefore we need to quantify how good it really is to be able to compare the results with other algorithms. We will cover more tools later in the class but now we will show the accuracy and the confusion matrix for our simple baseline model to evaluate how well it worked.


\subsection{Confusion Matrix}
\label{\detokenize{03-Datasets:confusion-matrix}}
\sphinxAtStartPar
The confusion matrix is a kind of histogram where you count the number of predicted occurances for each actual label. This gives us an idea about the classifyier performance.

\sphinxAtStartPar
We show which cases were most frequently confused


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
n=165
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Predicted TRUE
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Predicted FALSE
\\
\hline
\sphinxAtStartPar
\sphinxstylestrong{Actual TRUE}
&
\sphinxAtStartPar
\sphinxstyleemphasis{50}
&
\sphinxAtStartPar
10
\\
\hline
\sphinxAtStartPar
\sphinxstylestrong{Actual FALSE}
&
\sphinxAtStartPar
5
&
\sphinxAtStartPar
\sphinxstyleemphasis{100}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
This is only a simple matrix for the two cases \sphinxstyleemphasis{true} and \sphinxstyleemphasis{false}. The matrix does however grow with the number of classes in the data set. It is always a square matrix as we have the same number of actual classes as we have predicted classes.


\subsection{Confusion matrix for the MNIST classification}
\label{\detokenize{03-Datasets:confusion-matrix-for-the-mnist-classification}}
\sphinxAtStartPar
We saw that the k\sphinxhyphen{}nearest neighbors did a couple of missclassifications on the unseen test data. Now is the question how many mistakes it really does and how many correct labels it assigned. If compute the confusion matrix for this example, we will get a 10x10 matrix i.e. one for each class in the data set.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k}{def} \PYG{n+nf}{print\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{figsize} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Prints a confusion matrix, as returned by sklearn.metrics.confusion\PYGZus{}matrix, as a heatmap.}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Stolen from: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Arguments}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    confusion\PYGZus{}matrix: numpy.ndarray}
\PYG{l+s+sd}{        The numpy.ndarray object returned from a call to sklearn.metrics.confusion\PYGZus{}matrix. }
\PYG{l+s+sd}{        Similarly constructed ndarrays can also be used.}
\PYG{l+s+sd}{    class\PYGZus{}names: list}
\PYG{l+s+sd}{        An ordered list of class names, in the order they index the given confusion matrix.}
\PYG{l+s+sd}{    figsize: tuple}
\PYG{l+s+sd}{        A 2\PYGZhy{}long tuple, the first value determining the horizontal size of the ouputted figure,}
\PYG{l+s+sd}{        the second determining the vertical size. Defaults to (10,7).}
\PYG{l+s+sd}{    fontsize: int}
\PYG{l+s+sd}{        Font size for axes labels. Defaults to 14.}
\PYG{l+s+sd}{        }
\PYG{l+s+sd}{    Returns}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    matplotlib.figure.Figure}
\PYG{l+s+sd}{        The resulting confusion matrix figure}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{df\PYGZus{}cm} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}
        \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} 
    \PYG{p}{)}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}
    \PYG{k}{try}\PYG{p}{:}
        \PYG{n}{heatmap} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{df\PYGZus{}cm}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{except} \PYG{n+ne}{ValueError}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Confusion matrix values must be integers.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{heatmap}\PYG{o}{.}\PYG{n}{yaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}ticklabels}\PYG{p}{(}\PYG{n}{heatmap}\PYG{o}{.}\PYG{n}{yaxis}\PYG{o}{.}\PYG{n}{get\PYGZus{}ticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
    \PYG{n}{heatmap}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}ticklabels}\PYG{p}{(}\PYG{n}{heatmap}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{get\PYGZus{}ticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{45}\PYG{p}{,} \PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{ax1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{accuracy\PYGZus{}score}\PYG{p}{,} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{n}{pred\PYGZus{}values} \PYG{o}{=} \PYG{n}{neigh\PYGZus{}class}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{img}\PYG{p}{[}\PYG{l+m+mi}{24}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{o}{*}\PYG{l+m+mi}{28}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax1} \PYG{o}{=} \PYG{n}{print\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{l+m+mi}{24}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{pred\PYGZus{}values}\PYG{p}{)}\PYG{p}{,} \PYG{n}{class\PYGZus{}names}\PYG{o}{=}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Accuracy: }\PYG{l+s+si}{\PYGZob{}:2.2\PYGZpc{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{label}\PYG{p}{[}\PYG{l+m+mi}{24}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{pred\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03-Datasets_176_0}.png}

\sphinxAtStartPar
In this confusion matrix we see that some numbers are easier to classify than others. Some examples are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The ‘0’ seems to be hard to confuse with other numers.

\item {} 
\sphinxAtStartPar
Many images from all categories are falsely assigned to the ‘1’

\item {} 
\sphinxAtStartPar
The number ‘4’ is more probable to be assigned a label ‘9’ than the ‘4’

\end{itemize}

\sphinxAtStartPar
This experiment was done with a very limited training data set. You can experiment with more neighbors and more training data to see what improvement that brings. In all, there are 60000 images with digits in the data set.


\chapter{Summary}
\label{\detokenize{03-Datasets:summary}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The importance of good data

\item {} 
\sphinxAtStartPar
What is good data

\item {} 
\sphinxAtStartPar
Preparing data

\item {} 
\sphinxAtStartPar
Famous data sets

\item {} 
\sphinxAtStartPar
Augmentation
\begin{itemize}
\item {} 
\sphinxAtStartPar
Transformations for increase the data

\end{itemize}

\item {} 
\sphinxAtStartPar
Baseline algorithms
\begin{itemize}
\item {} 
\sphinxAtStartPar
What is it?

\item {} 
\sphinxAtStartPar
Why do we need it?

\item {} 
\sphinxAtStartPar
How good is our baseline algorithm?

\item {} 
\sphinxAtStartPar
The confusion matrix

\end{itemize}

\end{itemize}







\renewcommand{\indexname}{Index}
\printindex
\end{document}