{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction and Overview\n",
    "__Quantitative Big Imaging__ ETHZ: 227-0966-00L\n",
    "\n",
    "<p style=\"font-size:1em;\">February 25, 2021</p>\n",
    "<br /><br />\n",
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Anders Kaestner</p>  \n",
    "<p style=\"font-size:1em;\">Laboratory for Neutron Scattering and Imaging<br />Paul Scherrer Institut</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "\n",
    "- About the course\n",
    "- Motivating the use of quantitive methods in imaging\n",
    "- What is an image?\n",
    "- Where do images come from?\n",
    "- Science and Reproducibility\n",
    "- Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We need some python modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Python is a modular scripting language with limited functionality. Features are added through modules that are imported.\n",
    "These are the modules that are needed for this lecture. Please run this cell before you start using the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage import convolve\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "from itertools import product\n",
    "import os\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About the course\n",
    "\n",
    "- Who are we?\n",
    "- Who are you?\n",
    " - What is expected?\n",
    "- __Why does this class exist?__\n",
    " - Collection\n",
    " - Changing computing (Parallel / Cloud)\n",
    " - Course outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Who are we?\n",
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Anders Kaestner, PhD</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Anders Kaestner__\n",
    "\n",
    "You will hear me a lot during this course. I am the lecturer and I will also support you with problems during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/anders.jpeg\" style=\"height:250px\">\n",
    "\n",
    "- __Beamline scientist__ at the ICON Beamline at the SINQ (Neutron Source) at Paul Scherrer Institute\n",
    "    - __Lecturer__ at ETH Zurich\n",
    "- __Algorithm developer__ Varian Medical Systems, Baden-Daettwil\n",
    "- __Post Doc__ at ETH Zurich, Inst for Terrestial Ecology\n",
    "- __PhD__ at Chalmers Institute of Technology, Sweden, Signal processing\n",
    "\n",
    "\n",
    "anders.kaestner@psi.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Stefano van Gogh</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Stefano van Gogh__\n",
    "\n",
    "Will help you during the exercise sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " <img src=\"https://www.psi.ch/sites/default/files/styles/primer_teaser_square_scale/public/2019-06/picture.jpg?itok=t9wRh5Yb\" style=\"height:250px\">\n",
    "\n",
    "- __PhD Student__ in the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute\n",
    "- Teaching assistant\n",
    "\n",
    "stefano.van-gogh@psi.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Who are you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This course is targeting a wide range of students with different levels of experience. In the table you'll see were students came from in previos years. Some have a technical background others are merely producing images in the line of their project and have never seen much more than photoshop and similar programs for processing image data. Using some kind of programming is nescessary to perform quantitative image analysis on large data sets. A single or a few images can easily be handled with interactive software, but taking it beyond that is hard without writing some lines of code. \n",
    "\n",
    "Now, some of you have little to no programming experience while others have been programming since they got their first computer in the hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "|A wide spectrum of backgrounds|  A wide range of skills|\n",
    "|:---:|:---:|\n",
    "| Biomedical Engineers | |\n",
    "| Physicists | I think I've heard of python before|\n",
    "|Chemists | . |\n",
    "|Art History Researchers | .|\n",
    "|Mechanical Engineers | I write template C++ code and hand optimize it afterwards |\n",
    "| and Computer Scientists|  |\n",
    "\n",
    "<img src='figures/arrowsign.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So how will this ever work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now you maybe start to get worried! The purpose of this course is not to teach you programming but rather to provide you with a bag full recipes that you can use in your projects. Most of these recipes are just a list of the commands from different python moduls that you need to perform your analysis. A side-effect will probably be that you learn one or two programming tricks on the way.\n",
    "\n",
    "In the lectures, there will be small code pieces on the slides. Some of these are there to illustrate how an operation works, while other parts are there for the nice presentation of the results (this is mostly the second half of the code cell). Presenting the results is important. In the end, you want to show your results to the scinetific community. So even though the plotting clutters the slide, there is something to learn there as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Adaptive assignments__\n",
    "\n",
    "- Conceptual, graphical assignments with practical examples\n",
    "  - Emphasis on chosing correct steps and understanding workflow\n",
    "\n",
    "\n",
    "\n",
    "- Opportunities to create custom implementations, and perform more complicated analysis on larger datasets if interested\n",
    "  - Emphasis on performance, customizing analysis, and scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Course Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The practical part of the course has two parts. None of these are mandatory, but they will help you to better understand use the material you have learnt in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "| Exercises |\n",
    "|:---:|\n",
    "| Usually 1 set per lecture |\n",
    "| Optional (but recommended!) |\n",
    "| Easy - jupyter notebooks are prepared for the exercises |\n",
    "| Advanced - Writing Python, Java, C++, ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The exercises are prepared in a way that you learn step by step what you need to do and guids you through the problems. We will be using jupyter notebooks for the lectures. This is a very common way to work with image data these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "| Science project |\n",
    "|:---:|\n",
    "| Optional (but strongly recommended) |\n",
    "| Applying Techniques to answer scientific question! |\n",
    "| Ideally use on a topic relevant for your current project, thesis, or personal activities |\n",
    "| or choose from one of ours (will be online, soon) |\n",
    "| Present approach, analysis, and results |\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the optional science projects you will have to opportunity to test what you have learned during the course on real problems. This is the place for your creativity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projects\n",
    "\n",
    "- A small image processing project\n",
    "- Can be related to you Master or PhD project\n",
    "- You will get input and ideas for your own projects\n",
    "- You will get hands on experience on the techniques you learn here\n",
    "- Can be used as discussion base for your exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Course Overview\n",
    "\n",
    "|Topic| Date| Title | Description |\n",
    "|:---:|:---|:---|:---|\n",
    "| __Introduction__ | 25th February| Introduction and Workflows | Basic overview of the course, introduction to ...|\n",
    "| __Data__         | 4th March | Image Enhancement |\tOverview of what techniques are available for ...|\n",
    "|                  | 11th March | Ground Truth: Building and Augmenting Datasets | Examples of large datasets, how they were buil... |\n",
    "|__Segmentation__  | 18th March | Basic Segmentation, Discrete Binary Structures | How to convert images into structures, startin... |\t\n",
    "|                  | 25th March\t   | Advanced Segmentation\t| More advanced techniques for extracting struct... |\n",
    "|                  | 1st April      | Supervised Problems and Segmentation\t| More advanced techniques for extracting struct...|\n",
    "|                  | 8th April  | Easter break | Search for eggs|\n",
    "| __Analysis__     | 15th April\t| Analyzing Single Objects, Shape, and Texture |\tThe analysis and characterization of single st...|\n",
    "|                  | 22th April\t| Analyzing Complex Objects and Distributions\t| What techniques are available to analyze more ...|\n",
    "|                  | 29th April\t| Dynamic Experiments\t| Performing tracking and registration in dynami...|\n",
    "|__Big Imaging__   | 6th May\t| Imaging with multiple modalities | Combining information from different sources |\n",
    "|                  | 13th May\t| Ascension | Enjoy a lovely early summers day |\n",
    "|                  | 20th May   | Scaling Up / Big Data\t|Performing large scale analyses on clusters |\t\n",
    "|__Wrapping up__   | 27th May   | Project Presentations |\tYou present your projects|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's  Reading Material\n",
    "\n",
    "- Some book on image processing with python (to be updated)\n",
    "- Cloud Computing\n",
    " - [The Case for Energy-Proportional Computing](http://www-inst.eecs.berkeley.edu/~cs61c/sp14/) _ Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007_\n",
    " - [Concurrency](http://www.gotw.ca/publications/concurrency-ddj.htm)\n",
    "- Reproducibility\n",
    " - [Trouble at the lab](http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble) _Scientists like to think of science as self-correcting. To an alarming degree, it is not_\n",
    " - [Why is reproducible research important?](http://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/) _The Real Reason Reproducible Research is Important_\n",
    " - [Science Code Manifesto](http://software-carpentry.org/blog/2011/10/the-science-code-manifestos-five-cs.html)\n",
    " - [Reproducible Research Class](https://www.coursera.org/course/repdata) @ Johns Hopkins University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Literature / Useful References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These are books that are useful in many of the lectures. In particular the Image processing hand book by John Russ shows you an overview of typical image processing techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)\n",
    "    - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN)\n",
    "- Jean Claude, Morphometry with R\n",
    "    - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation - You have data!\n",
    "\n",
    "## Imaging experiments produce a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Working with imaging techniques you will get a lot of images that shows the sample in the eye of the technique you are using. The experiment were you acquire these images is only a small fraction of the complete workflow from idea to the final scientific publiction. The amout of data can also be overwhelming for many scientist with the consequence that the data is never analyzed properly, and then also not published in the way it really deserves. \n",
    "```{figure} figures/yougotdata.png\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "A typical imaging experiment produces large amounts of data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/yougotdata.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation - how to proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now is the question how to proceed towards a working analysis workflow that results in repeatable analyses for your data.\n",
    "```{figure} figures/crazyworkflow.png\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "A crazy unstructured and unclear work flow to analyze images from your experiment.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/crazyworkflow.png\">\n",
    "\n",
    "- To understand what, why and how from the moment an image is produced until it is finished (published, used in a report, …)\n",
    "- To learn how to go from one analysis on one image to 10, 100, or 1000 images (without working 10, 100, or 1000X harder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## High acquisition rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The trend in imaging is that experimentalist want to follow faster and faster processes. This wish can be supported the technical development of new detectors that provide very high acqisition rates. Here, we can also see that some cameras are able to produce more data than is uploaded per day on facebook and instagram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Detectors are getting bigger and faster constantly\n",
    "- Todays detectors are really fast\n",
    "    - 2560 x 2160 images @ 1500+ times a second = 8GB/s\n",
    "- Matlab / Avizo / Python / … are saturated after 60 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Many of the analysis platforms are already overwhelmed with handling the data rates produced by typical detector systems at imaging instrument. This restriction is partly due to hardware limitations. The memory is to small, hard drives are not sufficiently fast. The other side of the problem is that these tools are not prepared to work with large data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- A single camera\n",
    "    - [More information per day than Facebook](http://news.cnet.com/8301-1023_3-57498531-93/facebook-processes-more-than-500-tb-of-data-daily/)\n",
    "    - [Three times as many images per second as Instagram](http://techcrunch.com/2013/01/17/instagram-reports-90m-monthly-active-users-40m-photos-per-day-and-8500-likes-per-second/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different sources of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Images are produced by many different detectors and in some cases they are even the output from simulations. In the next sections we see some different imaging modalities and the data rates they produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### X-Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "X-ray imaging at syncrotron light sources produces very high frame rates thanks to the high brilliance of the source. Here are some examples of data rates from some instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - SRXTM images at (>1000fps) → 8GB/s\n",
    " - cSAXS diffraction patterns at 30GB/s\n",
    " - Nanoscopium Beamline, 10TB/day, 10-500GB file sizes\n",
    "\n",
    "### Optical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Optical imaging methods are more modest than the X-ray techniques, but still they produce data in the order of some hundred Mb per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - Light-sheet microscopy (see talk of Jeremy Freeman) produces images → 500MB/s\n",
    " - High-speed confocal images at (>200fps) → 78Mb/s\n",
    "\n",
    "### Personal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, we also take a look at cameras on the consumer market and see that these devices also produce relatively high data rates. This data must mostly be handled by normal household computers, which can be a challenging task..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- GoPro 4 Black - 60MB/s (3840 x 2160 x 30fps) for \\$600\n",
    "- [fps1000](https://www.kickstarter.com/projects/1623255426/fps1000-the-low-cost-high-frame-rate-camera) - 400MB/s (640 x 480 x 840 fps) for $400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The experiment life cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we have seen that there is a wish to obtain data at high rates and that there are technological solutions to provide this. The remainging part to develop is the post processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. __Experimental Design__ finding the right technique, picking the right dyes and samples has stayed relatively consistent, better techniques lead to more demanding scientists.\n",
    "\n",
    "2. __Measurements__ the actual acquisition speed of the data has increased wildly due to better detectors, parallel measurement, and new higher intensity sources\n",
    "\n",
    "3. __Management__ storing, backing up, setting up databases, these processes have become easier and more automated as data magnitudes have increased\n",
    "\n",
    "4. __Post Processing__ this portion has is the most time-consuming and difficult and has seen minimal improvements over the last years\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The post processing is the least trivial part to generalize. The initial steps are often posible to generalize as these are operations that all types of imaging experiments need to go through. When it comes to the experiment specific analysis the degree of generalization decreases and the scientists are left to develop their own procedure to extract the quantitative information from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "time-figure, fig.width=12, fig.height=8",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How is time used during the experiment life cycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "With the development of faster acquisision systems there has been a change in the ratio between \n",
    "- Exeperiment design and preparation\n",
    "- Measurements\n",
    "- Data management\n",
    "- and post processing\n",
    "\n",
    "over the years. This in particular the case for X-ray imaging where the flux is high and the acquisition is limited by the detector technology. In other modalities, where the measurement is flux limited we see a different distribution. \n",
    "\n",
    "```{figure} figures/qmia-014.png\n",
    "---\n",
    "scale: 50%\n",
    "--- \n",
    "The ratio of how much time is spent on different tasks during the lifecycle of an imaging experiment.\n",
    "```\n",
    "\n",
    "What also increases the post processing time is that the experiments have become more complicated over the years. Twenty years ago, it was sufficient to show qualitative information a beautify volume rendering or a movie of the sample. Meanwhile, it has become a requirement that you provide quantitative results from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "time-figure, fig.width=12, fig.height=8",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/qmia-014.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling masses of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### So... how much is a TB, really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We have been talking about different data amounts of MB, GB, and TB. But, what does that really mean in reality? Let us explore what is a TB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If __you__ looked at one image with 1000 x 1000 pixels (1 Mpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we create one image with 1000x1000 pixels with random values form a uniform distribution [0,1] and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.matshow(np.random.uniform(size = (1000, 1000)), \n",
    "           cmap = 'viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "every second, it would take you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# assuming 16 bit images and a 'metric' terabyte\n",
    "OneTB     = 1e12\n",
    "ImageSize = 1000*1000*16/8\n",
    "hour      = 60*60\n",
    "\n",
    "time_per_tb = OneTB/ImageSize/hour\n",
    "print(\"{0:0.1f} hours to view a terabyte\".format(time_per_tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overwhelmed scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Providing quantitative statements about image data is often very hard. You can may manage to do it on a single images like the bone image below.\n",
    "\n",
    "```{figure} figures/bone-cells.png\n",
    "---\n",
    "---\n",
    "A slice image show bone cells. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You would like to know:\n",
    "- Count how many cells are in the bone slice.\n",
    "        \n",
    "- Ignore the ones that are ‘too big’ or shaped ‘strangely’.\n",
    "        \n",
    "- Are there more on the right side or left side?\n",
    "        \n",
    "- Are the ones on the right or left bigger, top or bottom?\n",
    "\n",
    "<img src=\"figures/bone-cells.png\" style=\"height:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More overwhelmed scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Statistical analysis requires that you study many samples and not just a single one. The samples are also objects which requires 3D data instead of a single 2D slice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Many samples are needed:\n",
    "- Do it all over again for 96 more samples\n",
    "- This time in 3D with 2000 slices instead of just one!\n",
    "\n",
    "<img src=\"figures/96-samples.png\" style=\"height:50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/96-samples.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A collection of 96 volume images from different bone samples.\n",
    "``` \n",
    "Working with multiple 3D images is not feasible anymore to do manually. We need some kind of automated process to perform the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bring on the pain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The 96 samples only represented one of our cases in the study. Now, if we want to study different ages, healthy/diseased, etc, we need to add a sample batch for each case. Maybe we even need to increase the number of samples in each test group. With all these variations, we can easily end up in a thousand samples to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Great variations in the population   \n",
    "- Now again with 1090 samples!\n",
    "- How to measure?\n",
    "- How to analyze?\n",
    "\n",
    "<img src=\"figures/1090-samples.png\" style=\"height:50%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/1090-samples.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A collection of 1090 bone samples. This is a massive task to analyze!\n",
    "```\n",
    "With so many samples we stand in front of a logistic problem to measure the data and once the data is there we have to analyze it. As a first step, we have to specify how to analyze these images to obtain results that may or may not support a hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It gets better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The metrics we specified in the previous example are easy to observe and also to measure. They are direct measurements of pixels and positions. What if we now want to make more complicated inquiries even. Now how do we categorize the images or collections of features using soft metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Those metrics were quantitative and could be easily visually extracted from the images\n",
    "- What happens if you have _softer_ metrics\n",
    "    - How aligned are these cells?\n",
    "    - Is the group on the left more or less aligned than the right?\n",
    "    - errr?\n",
    "  \n",
    "<img src=\"figures/alignment-figure.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/alignment-figure.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Close-up on different bone segments. How aligned are the cells in these images?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Many experiments are on top of the spatial dimensions also studies over time. This brings us 4D data sets to analyze. How are we supposed to handle this? Looking at the movie we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- How many bubbles are here?\n",
    "- How fast are they moving?\n",
    "- Do they all move the same speed?\n",
    "- Do bigger bubbles move faster?\n",
    "- Do bubbles near the edge move slower?\n",
    "- Are they rearranging?\n",
    "\n",
    "<video controls loop src=\"movies/dk31_foam.mp4\" type=\"video/mp4\" height=\"350px\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images \n",
    "\n",
    "## An introduction to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### What is an image?\n",
    "\n",
    "A very abstract definition: \n",
    "- __A pairing between spatial information (position)__\n",
    "- __and some other kind of information (value).__\n",
    "\n",
    "In most cases this is a 2- or 3-dimensional position (x,y,z coordinates) and a numeric value (intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image sampling\n",
    "| The world is | The computer needs|\n",
    "|:---:|:---:|\n",
    "| Continuous    | Discrete levels |\n",
    "| No boundaries | Limited extent | \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/grid.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "The real world is sampled into discrete images with limited extent.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"figures/grid.svg\" style=\"height:400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What does sampling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "img=np.load('../../common/data/wood.npy');\n",
    "plt.figure(figsize=[15,7])\n",
    "plt.subplot(2,3,1); plt.imshow(img); plt.title('Original')\n",
    "downsize =  2; plt.subplot(2,3,2); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize), anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "downsize = 32; plt.subplot(2,3,3); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize),anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "levels   = 16; plt.subplot(2,3,5); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));\n",
    "levels   = 4 ; plt.subplot(2,3,6); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's create a small image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "basic_image = np.random.choice(range(100), size = (5,5))\n",
    "\n",
    "xx, yy   = np.meshgrid(range(basic_image.shape[1]), range(basic_image.shape[0]))\n",
    "image_df = pd.DataFrame(dict(x = xx.ravel(),\n",
    "                 y = yy.ravel(),\n",
    "                 Intensity = basic_image.ravel()))\n",
    "image_df[['x', 'y', 'Intensity']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(basic_image, cmap = 'gray')\n",
    "plt.colorbar(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 2D Intensity Images\n",
    "\n",
    "The next step is to apply a color map (also called lookup table, LUT) to the image \n",
    "- so it is a bit more exciting \n",
    "- some features are easier to detect [Rogowitz et al. 1996](https://doi.org/10.1063/1.4822401)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "plot_image = ax1.matshow(basic_image, cmap = 'Blues')\n",
    "plt.colorbar(plot_image)\n",
    "\n",
    "for _, c_row in image_df.iterrows():\n",
    "    ax1.text(c_row['x'], c_row['y'], s = '%02d' % c_row['Intensity'], fontdict = dict(color = 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different colormaps\n",
    "\n",
    "Color maps can be arbitrarily defined based on how we would like to visualize the information in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_image = plt.matshow(basic_image, cmap = 'jet')\n",
    "plt.colorbar(plot_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_image = plt.matshow(basic_image, cmap = 'hot')\n",
    "plt.colorbar(plot_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Lookup Tables\n",
    "\n",
    "Formally a color map is lookup table or a function which\n",
    "$$ f(\\textrm{Intensity}) \\rightarrow \\textrm{Color} $$\n",
    "\n",
    "#### Matplotlib's color maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "#from colorspacious import cspace_converter\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict() \n",
    "cmaps['Perceptually Uniform Sequential'] = [\n",
    "            'viridis', 'plasma', 'inferno', 'magma', 'cividis']\n",
    "\n",
    "cmaps['Sequential'] = [\n",
    "            'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "cmaps['Sequential (2)'] = [\n",
    "            'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "            'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "            'hot', 'afmhot', 'gist_heat', 'copper']\n",
    "\n",
    "cmaps['Diverging'] = [\n",
    "            'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "            'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']\n",
    "\n",
    "cmaps['Cyclic'] = ['twilight', 'twilight_shifted', 'hsv']\n",
    "\n",
    "cmaps['Qualitative'] = ['Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                        'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                        'tab10', 'tab20', 'tab20b', 'tab20c']\n",
    "\n",
    "cmaps['Miscellaneous'] = [\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar']\n",
    "\n",
    "\n",
    "nrows = max(len(cmap_list) for cmap_category, cmap_list in cmaps.items())\n",
    "\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list, nrows):\n",
    "    fig, axes = plt.subplots(nrows=nrows)\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.2, right=0.99)\n",
    "    axes[0].set_title(cmap_category + ' colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axes, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        pos = list(ax.get_position().bounds)\n",
    "        x_text = pos[0] - 0.01\n",
    "        y_text = pos[1] + pos[3]/2.\n",
    "        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "for cmap_category, cmap_list in cmaps.items():\n",
    "    plot_color_gradients(cmap_category, cmap_list, nrows)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<center>Never use rainbox maps like jet, see <a href=\"https://agilescientific.com/blog/2017/12/14/no-more-rainbows\">No more rainbows!</a></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How are the colors combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(0, 1, 100)\n",
    "colors = ['Red','Green','Blue']\n",
    "plt.figure(figsize=[15,4])\n",
    "for i in np.arange(0,3) :\n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.hot(xlin)[:,i],\n",
    "            c = plt.cm.hot(xlin),label=\"hot\")\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.Blues(xlin)[:,i], \n",
    "            c = plt.cm.Blues(xlin),label=\"blues\")\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.jet(xlin)[:,i], \n",
    "            c = plt.cm.jet(xlin),label='jet')\n",
    "\n",
    "    plt.xlabel('Intensity');\n",
    "    plt.ylabel('{0} Component'.format(colors[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applied LUTs\n",
    "These transformations can also be non-linear as is the case of the graph below where the mapping between the intensity and the color is a $\\log$ relationship meaning the the difference between the lower values is much clearer than the higher ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.logspace(-2, 5, 500)\n",
    "log_xlin = np.log10(xlin)\n",
    "norm_xlin = (log_xlin-log_xlin.min())/(log_xlin.max()-log_xlin.min())\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(norm_xlin)[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(xlin/xlin.max())[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "ax1.set_xscale('log');ax1.set_xlabel('Intensity');ax1.set_ylabel('Red Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LUTs on real images\n",
    "\n",
    "On a real image the difference is even clearer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (12, 4))\n",
    "in_img = imread('figures/bone-section.png')[:,:,0].astype(np.float32)\n",
    "ax1.imshow(in_img, cmap = 'gray');\n",
    "ax1.set_title('grayscale LUT');\n",
    "\n",
    "ax2.imshow(in_img, cmap = 'hot');\n",
    "ax2.set_title('hot LUT');\n",
    "\n",
    "ax3.imshow(np.log2(in_img+1), cmap = 'gray');\n",
    "ax3.set_title('grayscale-log LUT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3D Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For a 3D image, the position or spatial component has a 3rd dimension (z if it is a spatial, or t if it is a movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<table><tr><td>Volume</td><td>Time series</td></tr>\n",
    "    <tr>\n",
    "    <td><img src=\"figures/cube_10x10x10.svg\"></td>\n",
    "    <td><img src=\"figures/timeseries_visualization.svg\">\n",
    "</td><tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/cube_10x10x10.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Three-dimensional data can be a volume in space.\n",
    "```\n",
    "\n",
    "```{figure} figures/timeseries_visualization.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A movie can also be seen as a three-dimensional image.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A 3D image as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vol_image = np.arange(27).reshape((3,3,3))\n",
    "print(vol_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Showing 2D slices from volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=10",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import montage as montage2d\n",
    "print(montage2d(vol_image, fill = 0))\n",
    "plt.matshow(montage2d(vol_image, fill = 0), cmap = 'jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Values per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the images thus far, we have had one value per position, but there is no reason there cannot be multiple values. In fact this is what color images are (red, green, and blue) values and even 4 channels with transparency (alpha) as a different. For clarity we call the __dimensionality__ of the image the number of dimensions in the spatial position, and the __depth__ the number in the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base_df = pd.DataFrame([dict(x = x, y = y) for x,y in product(range(5), range(5))])\n",
    "base_df['Intensity'] = np.random.uniform(0, 1, 25)\n",
    "base_df['Transparency'] = np.random.uniform(0, 1, 25)\n",
    "base_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Display multi-valued pixels separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The most straight forward way to display multiple pixel values is to display each value separately. This method is, however, mostly not very suitable as the values often are related in some sense. Therefore it is recommended to combine the values in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=8, fig.width = 6",
    "autoscroll": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Intensity']), s = 1000)\n",
    "ax1.set_title('Intensity')\n",
    "ax2.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Transparency']), s = 1000)\n",
    "ax2.set_title('Transparency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we combined two values use one value to control the colormap and the other to control the size of the dots. How you combine the data is related to the to of data you want to combine. If the values are components of a vector it makes more sense to show arrows of different length and direction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.jet(base_df['Intensity']), s = 1000*base_df['Transparency'])\n",
    "ax1.set_title('Intensity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperspectral Imaging\n",
    "\n",
    "\n",
    "At each point in the image (black dot), instead of having just a single value, there is an entire spectrum. A selected group of these (red dots) are shown to illustrate the variations inside the sample. While certainly much more complicated, this still constitutes and image and requires the same sort of techniques to process correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "load_hypermap",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import os\n",
    "raw_img = imread('../../common/data/raw.jpg')\n",
    "im_pos = pd.read_csv('../../common/data/impos.csv', header = None)\n",
    "im_pos.columns = ['x', 'y']\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 8));\n",
    "ax1.imshow(raw_img);\n",
    "ax1.scatter(im_pos['x'], im_pos['y'], s = 1, c = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Looking at the pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Each pixel of this data set is represented by a full spectrum. This means that we have both wave numbers and intensity values. In many cases, the wavenumbers are the same for all pixels, which makes it possible to reduce the redundancy of the wave number vector using a single 1D array to represent this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../../common/data/full_img.csv').query('wavenum<1200')\n",
    "print(full_df.shape[0], 'rows')\n",
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "full_df['g_x'] = pd.cut(full_df['x'], 5)\n",
    "full_df['g_y'] = pd.cut(full_df['y'], 5)\n",
    "fig, m_axs = plt.subplots(9, 3, figsize = (15, 12)); m_axs=m_axs.ravel()\n",
    "for ((g_x, g_y), c_rows), c_ax in zip(full_df.sort_values(['x','y']).groupby(['g_x', 'g_y']),m_axs):\n",
    "    c_ax.plot(c_rows['wavenum'], c_rows['val'], 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The image formation process is the process to use some kind of excitation or impulse probe a sample. This requires the interaction of four parts.\n",
    " ```{figure} figures/image-formation.pdf\n",
    " ---\n",
    " ---\n",
    " The parts involved in the image formation process probing a sample.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/image-formation.svg\" />\n",
    "\n",
    "- __Impulses__ Light, X-Rays, Electrons, A sharp point, Magnetic field, Sound wave\n",
    "- __Characteristics__ Electron Shell Levels, Electron Density, Phonons energy levels, Electronic, Spins, Molecular mobility\n",
    "- __Response__ Absorption, Reflection, Phase Shift, Scattering, Emission\n",
    "- __Detection__ Your eye, Light sensitive film, CCD / CMOS, Scintillator, Transducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where do images come from?\n",
    "\n",
    "\n",
    "\n",
    "| Modality | Impulse | Characteristic | Response | Detection |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Light Microscopy| White Light | Electronic interactions | Absorption |Film, Camera |\n",
    "| Phase Contrast | Coherent light | Electron Density (Index of Refraction) | Phase Shift | Phase stepping, holography, Zernike |\n",
    "| Confocal Microscopy |Laser Light | Electronic Transition in Fluorescence Molecule | Absorption and reemission |Pinhole in focal plane, scanning detection |\n",
    "| X-Ray Radiography | X-Ray light | Photo effect and Compton scattering | Absorption and scattering | Scintillator, microscope, camera |\n",
    "| Neutron Radiography | Neutrons | Interaction with nucleus |Scattering and absorption| Scintillator, optics, camera |\n",
    "| Ultrasound |High frequency sound waves | Molecular mobility | Reflection and Scattering | Transducer |\n",
    "| MRI | Radio-frequency EM | Unmatched Hydrogen spins | Absorption and reemission | RF coils to detect |\n",
    "| Atomic Force Microscopy | Sharp Point | Surface Contact | Contact, Repulsion | Deflection of a tiny mirror|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Acquiring Images\n",
    "\n",
    "### Traditional / Direct imaging\n",
    "- Visible images produced or can be easily made visible\n",
    "- Optical imaging, microscopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\" here the measurement is supposed to be from a typical microscope which blurs, flips and otherwise distorts the image but the original representation is still visible\"",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bone_img  = imread('figures/tiny-bone.png').astype(np.float32)\n",
    "# simulate measured image\n",
    "conv_kern = np.pad(disk(2), 1, 'constant', constant_values = 0)\n",
    "meas_img  = convolve(bone_img[::-1], conv_kern)\n",
    "# run deconvolution\n",
    "dekern    = np.fft.ifft2(1/np.fft.fft2(conv_kern))\n",
    "rec_img   = convolve(meas_img, dekern)[::-1]\n",
    "# show result\n",
    "fig, (ax_orig, ax1, ax2) = plt.subplots(1,3, figsize = (15, 5))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone'); ax_orig.set_title('Original Object')\n",
    "\n",
    "ax1.imshow(np.real(meas_img), cmap = 'bone'); ax1.set_title('Measurement')\n",
    "\n",
    "ax2.imshow(np.real(rec_img), cmap = 'bone', vmin = 0, vmax = 255); ax2.set_title('Reconstructed');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indirect / Computational imaging\n",
    "- Recorded information does not resemble object\n",
    "- Response must be transformed (usually computationally) to produce an image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\"here the measurement is supposed to be from a diffraction style experiment where the data is measured in reciprocal space (fourier) and can be reconstructed to the original shape\"",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bone_img = imread('figures/tiny-bone.png').astype(np.float32)\n",
    "# simulate measured image\n",
    "meas_img = np.log10(np.abs(np.fft.fftshift(np.fft.fft2(bone_img))))\n",
    "print(meas_img.min(), meas_img.max(), meas_img.mean())\n",
    "fig, (ax1, ax_orig) = plt.subplots(1,2, \n",
    "                               figsize = (12, 6))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone')\n",
    "ax_orig.set_title('Original Object')\n",
    "\n",
    "ax1.imshow(meas_img, cmap = 'hot')\n",
    "ax1.set_title('Measurement');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traditional Imaging\n",
    "\n",
    "\n",
    "<img src=\"figures/traditional-imaging.png\" style=\"height:500px\"/>\n",
    "\n",
    "\n",
    "<small>\n",
    "Copyright 2003-2013 J. Konrad in EC520 lecture, reused with permission\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional Imaging: Model\n",
    "\n",
    "\n",
    "![Traditional Imaging Model](figures/traditional-image-flow.png)\n",
    "\n",
    "$$\n",
    "\\left[\\left([b(x,y)*s_{ab}(x,y)]\\otimes h_{fs}(x,y)\\right)*h_{op}(x,y)\\right]*h_{det}(x,y)+d_{dark}(x,y)\n",
    "$$\n",
    "\n",
    "$s_{ab}$ is the only information you are really interested in, so it is important to remove or correct for the other components\n",
    "\n",
    "For color (non-monochromatic) images the problem becomes even more complicated\n",
    "$$\n",
    "\\int_{0}^{\\infty} {\\left[\\left([b(x,y,\\lambda)*s_{ab}(x,y,\\lambda)]\\otimes h_{fs}(x,y,\\lambda)\\right)*h_{op}(x,y,\\lambda)\\right]*h_{det}(x,y,\\lambda)}\\mathrm{d}\\lambda+d_{dark}(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indirect Imaging (Computational Imaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "With indirect imaging you make acquisitions in a form that don't represent the information you want to have. It is needed to perform a numeric transformation to obtain images in observation space.\n",
    "\n",
    "Some examples are:\n",
    "- Tomography through projections\n",
    "- Microlenses [Light-field photography](https://en.wikipedia.org/wiki/Light-field_camera)\n",
    "- Diffraction patterns\n",
    "- Hyperspectral imaging with Raman, IR, CARS\n",
    "- Surface Topography with cantilevers (AFM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<tr><td>\n",
    "    \n",
    "- Tomography through projections\n",
    "- Microlenses [Light-field photography](https://en.wikipedia.org/wiki/Light-field_camera)\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- Diffraction patterns\n",
    "- Hyperspectral imaging with Raman, IR, CARS\n",
    "- Surface Topography with cantilevers (AFM)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><video controls loop src=\"movies/lightfield.mp4\" height=\"300px\" type=\"video/mp4\"></video></td>\n",
    "<td><img src=\"figures/surface-plot.png\" style=\"height:300px\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different views on image Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Image analysis is a complex task and there are many ways to reach the quantitative results from the images. \n",
    "```{figures} figures/approaches.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Your background often decides how you approach an image analysis problem.\n",
    "```\n",
    "We can make two initial statements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"figures/approaches.png\" style=\"height:400px\"></center>\n",
    "\n",
    "- An image is a bucket of pixels.\n",
    "- How you choose to turn it into useful information is strongly dependent on your background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Experimentalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The experimentalist looks with a problem driven concept on the analysis task. It is often a top down approach aiming at solving the specific problem at hand. The solution is often reality driven and aims at finding models explaining the information presented in the images.\n",
    "\n",
    "Typical task the experimentalist tries to solve are very practical and specific like counting cells in the image or to measure the porosity of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table>\n",
    "    <tr><td align=\"left\"> \n",
    "    \n",
    "### Characteristics\n",
    "-  Problem-driven \n",
    "- Top-down \n",
    "- _Reality_ Model-based \n",
    "\n",
    "### Examples\n",
    "- cell counting\n",
    "- porosity\n",
    "</td>\n",
    "<td><img src=\"figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Computer Vision Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The computer vision/signal processing scientist works to develop methods to solve a class of image processing problem. The approach is based on abstract features found in the image. The models are based on features and noise found in the images. The systematic appoach is even based on engineered image features to better test and evaluate the developed methods.\n",
    "\n",
    "The computer vision approach is typical looking to detect features like edges, structures, and also complicated features like faces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table>\n",
    "    <tr><td align=\"left\"> \n",
    "\n",
    "### Characteristics\n",
    "- Method-driven\n",
    " - Feature-based\n",
    " - _Image_ Model-based\n",
    "- Engineer features for solving problems\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Edge detection\n",
    "- Face detection\n",
    "</td>\n",
    "<td>\n",
    "<td><img src=\"figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, the deep learning approach is data driven and inspired by the way nature solves the image analysis problem. This approach rebuilds the way image processing is done from scratch, but at the same time it also based on concepts developed in computer vision. The deep learning approaches doesn't require a specific model describing the images it is meant to analyze, but rather make conclusions based on the previous images it has been exposed to. \n",
    "\n",
    "The deep learning appraoch is good handling rare events in the data and when it trained correctly it is also capable of generalizing to detect new features. This may sound like magic, but this is also a well founded and structured approach to analyzing images. Care most however be taken not to over fit or to genralize to much. The models are never better than the data they have been exposed to.\n",
    "\n",
    "Examples where deep learing is frequently used are to detect annomalies in the data or to label images based on their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            \n",
    "### Characteristics\n",
    "- Results-driven\n",
    "- Biology ‘inspired’\n",
    "- Build both image processing and analysis from scratch\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Captioning images\n",
    "- Identifying unusual events\n",
    "</td>\n",
    "<td><img src=\"figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Summary analysis approaches\n",
    "\n",
    "These three approaches have their own advantages and disadvantages, therefore it is good to know them all to be able to select the adquate method for the task you have to solve. It is not unusual that you will have to use a mix of the approaches. It is important to be open minded and think outside the box. In the end, what matters is that you can provide a reliable analysis of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On Science\n",
    "\n",
    "## What is the purpose?\n",
    "\n",
    "\n",
    "- Discover and validate new knowledge\n",
    "\n",
    "### How?\n",
    "- Use the scientific method as an approach to convince other people\n",
    "- Build on the results of others so we don't start from the beginning\n",
    "\n",
    "### Important Points\n",
    "- While __qualitative__ assessment is important, it is difficult to reliably produce and scale\n",
    "- __Quantitative__ analysis is far from perfect, but provides metrics which can be compared and regenerated by anyone\n",
    "\n",
    "<small>Inspired by: [imagej-pres](http://www.slideshare.net/CurtisRueden/imagej-and-the-scijava-software-stack)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Science and Imaging\n",
    "Images are great for qualitative analyses since our brains can quickly interpret them without large _programming_ investements.\n",
    "### Proper processing and quantitative analysis is however much more difficult with images.\n",
    " - If you measure a temperature, quantitative analysis is easy, $50K$.\n",
    " - If you measure an image it is much more difficult and much more prone to mistakes, subtle setup variations, and confusing analyses\n",
    "\n",
    "\n",
    "### Furthermore in image processing there is a plethora of tools available\n",
    "\n",
    "- Thousands of algorithms available\n",
    "- Thousands of tools\n",
    "- Many images require multi-step processing\n",
    "- Experimenting is time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why quantitative?\n",
    "\n",
    "### Human eyes have issues\n",
    "\n",
    "Which center square seems brighter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.align='center'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 3)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "img_a = 25*np.ones((3,3))\n",
    "img_b = np.ones((3,3))*75\n",
    "img_a[1,1] = 50\n",
    "img_b[1,1] = 50\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "ax1.matshow(img_a, vmin = 0, vmax = 100, cmap = 'bone');\n",
    "ax2.matshow(img_b, vmin = 0, vmax = 100, cmap = 'bone');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Intensity gradients\n",
    "Are the intensities constant in the image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 10)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6))\n",
    "ax1.matshow(xx, vmin = -1, vmax = 1, cmap = 'bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reproducibility vs. Repeatability\n",
    "\n",
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/reproducibility.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A workflow describing the concept of reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figures/reproducibility.svg' style='height:300px'>\n",
    "\n",
    "### Repeatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/repeatability.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A workflow describing the concept of reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figures/repeatability.svg' style='height:300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reproducibility vs. Repeatability\n",
    "\n",
    "Science demands __repeatability__! and really wants __reproducability__\n",
    "- Experimental conditions can change rapidly and are difficult to make consistent\n",
    "- Animal and human studies are prohibitively time consuming and expensive to reproduce\n",
    "- Terabyte datasets cannot be easily passed around many different groups\n",
    "- Privacy concerns can also limit sharing and access to data\n",
    "\n",
    "----\n",
    "\n",
    "- _Science_ is already difficult enough\n",
    "- Image processing makes it even more complicated\n",
    "- Many image processing tasks are multistep, have many parameters, use a variety of tools, and consume a very long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we keep track of everything for ourselves and others?\n",
    "- We can make the data analysis easy to repeat by an independent 3rd party\n",
    "- Document the analysis steps\n",
    "- Write clear and understandable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workflows for image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computing has changed: Parallel\n",
    "\n",
    "\n",
    "## Moores Law\n",
    "$$ \\textrm{Transistors} \\propto 2^{T/(\\textrm{18 months})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# stolen from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d\n",
    "moores_txt=[\"Id Name  Year  Count(1000s)  Clock(MHz)\\n\",\n",
    "        \"0            MOS65XX  1975           3.51           14\\n\",\n",
    "        \"1          Intel8086  1978          29.00           10\\n\",\n",
    "        \"2          MIPSR3000  1988         120.00           33\\n\",\n",
    "        \"3           AMDAm486  1993        1200.00           40\\n\",\n",
    "        \"4        NexGenNx586  1994        3500.00          111\\n\",\n",
    "        \"5          AMDAthlon  1999       37000.00         1400\\n\",\n",
    "        \"6   IntelPentiumIII  1999       44000.00         1400\\n\",\n",
    "        \"7         PowerPC970  2002       58000.00         2500\\n\",\n",
    "        \"8       AMDAthlon64  2003      243000.00         2800\\n\",\n",
    "        \"9    IntelCore2Duo  2006      410000.00         3330\\n\",\n",
    "        \"10         AMDPhenom  2007      450000.00         2600\\n\",\n",
    "        \"11      IntelCorei7  2008     1170000.00         3460\\n\",\n",
    "        \"12      IntelCorei5  2009      995000.00         3600\"]\n",
    "\n",
    "sio_table = StringIO(''.join(moores_txt)); moore_df = pd.read_table(sio_table, sep = '\\s+', index_col = 0);\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 4)); ax1.semilogy(moore_df['Year'], moore_df['Count(1000s)'], 'b.-', label = '1000s of transitiors'); ax1.semilogy(moore_df['Year'], moore_df['Clock(MHz)'], 'r.-', label = 'Clockspeed (MHz)') ;ax1.legend(loc = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>_Based on data from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d_</small>\n",
    "\n",
    "----\n",
    "\n",
    "There are now many more transistors inside a single computer but the processing speed hasn't increased. How can this be?\n",
    "\n",
    "- Multiple Core\n",
    " - Many machines have multiple cores for each processor which can perform tasks independently\n",
    "- Multiple CPUs\n",
    " - More than one chip is commonly present\n",
    "- New modalities\n",
    "  - GPUs provide many cores which operate at slow speed\n",
    "\n",
    "### $\\rightarrow$ Parallel Code is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing has changed: Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Computer, servers, workstations are _wildly underused_ (majority are <50%)\n",
    "- Buying a big computer that sits _idle most of the time_ is a waste of money\n",
    "    - http://www-inst.eecs.berkeley.edu/~cs61c/sp14/\n",
    "    - “The Case for Energy-Proportional Computing,” Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007\n",
    "- Traditionally the most important performance criteria was time, how fast can it be done\n",
    "- With Platform as a service servers can be _rented instead of bought_\n",
    "- Speed is still important but using cloud computing $ / Sample is the real metric\n",
    "- In Switzerland a PhD student is 400x as expensive per hour as an Amazon EC2 Machine\n",
    "- Many competitors keep prices low and offer flexibility\n",
    "\n",
    "\n",
    "<img src=\"figures/cloud-services.png\" style=\"height:600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Soup/Recipe Example\n",
    "\n",
    "### Simple Soup\n",
    "Easy to follow the list, anyone with the right steps can execute and repeat (if not reproduce) the soup\n",
    "\n",
    "1. Buy {carrots, peas, tomatoes} at market\n",
    "1. _then_ Buy meat at butcher\n",
    "1. _then_ Chop carrots into pieces\n",
    "1. _then_ Chop potatos into pieces\n",
    "1. _then_ Heat water\n",
    "1. _then_ Wait until boiling then add chopped vegetables\n",
    "1. _then_ Wait 5 minutes and add meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More complicated soup\n",
    "Here it is harder to follow and you need to carefully keep track of what is being performed\n",
    "\n",
    "#### Steps 1-4\n",
    "4. _then_ Mix carrots with potatos $\\rightarrow  mix_1$\n",
    "4. _then_ add egg to $mix_1$ and fry for 20 minutes\n",
    "4. _then_ Tenderize meat for 20 minutes\n",
    "4. _then_ add tomatoes to meat and cook for 10 minutes $\\rightarrow mix_2$\n",
    "5. _then_ Wait until boiling then add $mix_1$\n",
    "6. _then_ Wait 5 minutes and add $mix_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using flow charts / workflows\n",
    "\n",
    "### Simple Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n) \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "for (c_n, d_n) in zip(nodes, nodes[1:]):\n",
    "    graph.add_edge(pydot.Edge(c_n, d_n))\n",
    "\n",
    "SVG(graph.create_svg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Workflows\n",
    "\n",
    "Clearly a linear set of instructions is ill-suited for even a fairly easy soup, it is then even more difficult when there are dozens of steps and different pathsways\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Furthermore a clean workflow allows you to better parallelize the task since it is clear which tasks can be performed independently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=9",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n, style = 'filled') \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "def e(i,j, col = None):\n",
    "    if col is not None:\n",
    "        for c in [i,j]:\n",
    "            if nodes[c].get_fillcolor() is None: \n",
    "                nodes[c].set_fillcolor(col)\n",
    "    graph.add_edge(pydot.Edge(nodes[i], nodes[j]))\n",
    "\n",
    "e(0, 2, 'gold'); e(2, 4); e(3, -2, 'springgreen'); e(-2, 4, 'orange'); e(4, -1) ; e(1, -1, 'dodgerblue')\n",
    "\n",
    "SVG(graph.create_svg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Directed Acyclical Graphs (DAG)\n",
    "We can represent almost any computation without loops as DAG. What this allows us to do is now break down a computation into pieces which can be carried out independently. There are a number of tools which let us handle this issue.\n",
    "\n",
    "- PyData Dask - https://dask.pydata.org/en/latest/\n",
    "- Apache Spark - https://spark.apache.org/\n",
    "- Spotify Luigi - https://github.com/spotify/luigi\n",
    "- Airflow - https://airflow.apache.org/\n",
    "- KNIME - https://www.knime.com/\n",
    "- Google Tensorflow - https://www.tensorflow.org/\n",
    "- Pytorch / Torch - http://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concrete example\n",
    "What is a DAG good for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((5,5), chunks = (5,5))\n",
    "image_2 = da.ones((5,5), chunks = (5,5))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_3 = image_1 + image_2\n",
    "dot_graph(image_3.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's go big\n",
    "Now let's see where this can be really useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((1024, 1024), chunks = (512, 512))\n",
    "image_2 = da.ones((1024 ,1024), chunks = (512, 512))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_5 = da.matmul(image_1, image_2)\n",
    "dot_graph(image_5.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_6 = (da.matmul(image_1, image_2)+image_1)*image_2\n",
    "dot_graph(image_6.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask_ndfilters as da_ndfilt\n",
    "image_7 = da_ndfilt.convolve(image_6, image_1)\n",
    "dot_graph(image_7.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Deep Learning\n",
    "We won't talk too much about deep learning now, but it certainly shows why DAGs are so important. \n",
    "\n",
    "The steps above are simple toys compared to what tools are already in use for machine learning\n",
    "\n",
    "https://keras.io/api/utils/model_plotting_utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this lecture we saw that:\n",
    "- Images revieal information about different samples\n",
    "- Images are a signals that needs to be quantitatively analyzed\n",
    "- Science with images is a non-trivial task\n",
    "- Proper workflows are required for efficient analysis repeatable analysis."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "livereveal": {
   "autolaunch": true,
   "footer": "February 20, 2020 - ETH 227-0966-00L: Quantitative Big Imaging - Introduction",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
